{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 28,
   "id": "8f2c2764-577e-4a4f-b298-0143170cfffe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> Stashed changes
   "execution_count": 1,
   "id": "f7385ecb-6a10-45a7-8fcf-37463ddeb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
<<<<<<< Updated upstream
    "df = pd.read_csv(\"Inputs1stGen.csv\", low_memory = False)\n",
    "df = df.dropna()"
=======
    "df = pd.read_csv(\"Inputs1stGen.csv\", low_memory = False)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f066768-e4be-4c6b-b2ef-5a5e11cf78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      by   karma  \\\n",
      "0            ColinWright  127765   \n",
      "1                   hncj      44   \n",
      "2             andymboyle     714   \n",
      "3                digisth    2395   \n",
      "4                jazzdev     543   \n",
      "...                  ...     ...   \n",
      "5116173   sandwichsphinx      14   \n",
      "5116174       emperinter       7   \n",
      "5116175       mellosouls   14490   \n",
      "5116176          d4vinci       1   \n",
      "5116177  giuliomagnifico   40731   \n",
      "\n",
      "                                                     title  \\\n",
      "0        \"What May Happen in the Next Hundred Years\", f...   \n",
      "1             Getting Started with JavaScript Unit Testing   \n",
      "2        Armstrong, the Django-based and open-source ne...   \n",
      "3                     Why Web Reviewers Make Up Bad Things   \n",
      "4        You Weren't Meant to Have a Boss: The Cliff Notes   \n",
      "...                                                    ...   \n",
      "5116173                         Northeast Blackout of 2003   \n",
      "5116174  PieChartMaster- Unlock your Pie/Rose chart cre...   \n",
      "5116175                  The Greatest Checkmate Ever Given   \n",
      "5116176  Scrapling: Fast, Adaptive Web Scraping for Python   \n",
      "5116177                       Monkeys Predict US Elections   \n",
      "\n",
      "                                                       url  \\\n",
      "0        http://www.howtobearetronaut.com/wp-content/up...   \n",
      "1        http://blogs.lessthandot.com/index.php/WebDev/...   \n",
      "2        http://www.marketwatch.com/story/the-bay-citiz...   \n",
      "3        http://bits.blogs.nytimes.com/2013/07/15/why-w...   \n",
      "4                     http://paulgraham.com/bossnotes.html   \n",
      "...                                                    ...   \n",
      "5116173  https://en.wikipedia.org/wiki/Northeast_blacko...   \n",
      "5116174  https://apps.apple.com/us/app/piechartmaster-u...   \n",
      "5116175        https://www.youtube.com/watch?v=UULlFap1Zko   \n",
      "5116176               https://github.com/D4Vinci/Scrapling   \n",
      "5116177  https://www.biorxiv.org/content/10.1101/2024.0...   \n",
      "\n",
      "                        time  score  \n",
      "0        2011-10-24 16:27:00     19  \n",
      "1        2012-01-23 11:39:25      1  \n",
      "2        2011-10-24 16:27:36      2  \n",
      "3        2013-07-16 05:16:26      1  \n",
      "4        2008-03-30 09:46:25      1  \n",
      "...                      ...    ...  \n",
      "5116173  2024-10-13 23:41:46      1  \n",
      "5116174  2024-10-13 23:42:21      1  \n",
      "5116175  2024-10-13 23:45:21      1  \n",
      "5116176  2024-10-13 23:49:42      1  \n",
      "5116177  2024-10-13 23:53:00      1  \n",
      "\n",
      "[4603891 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c577c4c2-baa0-4743-ae47-d5404b4be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "'''\n",
    "The following section gives us all the parameters relating to time:\n",
    "1. Day of week\n",
    "2. Time of day\n",
    "3. Month of Year\n",
    "4. Year of past \n",
    "'''\n",
    "\n",
    "#Converting UNIX timestamp to datetime\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "\n",
    "#Getting the names of the days of the week\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "\n",
    "#Creating a key of days of the week to numbers\n",
    "day_map = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5, 'Sunday':6\n",
    "}\n",
    "\n",
    "#Assigning numbers to the days of the week corresponding to the dataset\n",
    "df[\"day_of_week_num\"] = df['day_of_week'].map(day_map)\n",
    "\n",
    "#Getting the names of the months of year\n",
    "df['month'] = df['datetime'].dt.month_name()\n",
    "\n",
    "#Creating a key of months of the year to numbers\n",
    "month_map = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April':4, 'May': 5, 'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October':10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "df['month_num'] = df['month'].map(month_map)\n",
    "\n",
    "#Getting the hour of the post\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "#Getting the year of the post\n",
    "df['year'] = df['datetime'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26532795-6b37-4dd8-8bdd-a6e78cb43e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      by   karma  \\\n",
      "0            ColinWright  127765   \n",
      "1                   hncj      44   \n",
      "2             andymboyle     714   \n",
      "3                digisth    2395   \n",
      "4                jazzdev     543   \n",
      "...                  ...     ...   \n",
      "5116173   sandwichsphinx      14   \n",
      "5116174       emperinter       7   \n",
      "5116175       mellosouls   14490   \n",
      "5116176          d4vinci       1   \n",
      "5116177  giuliomagnifico   40731   \n",
      "\n",
      "                                                     title  \\\n",
      "0        \"What May Happen in the Next Hundred Years\", f...   \n",
      "1             Getting Started with JavaScript Unit Testing   \n",
      "2        Armstrong, the Django-based and open-source ne...   \n",
      "3                     Why Web Reviewers Make Up Bad Things   \n",
      "4        You Weren't Meant to Have a Boss: The Cliff Notes   \n",
      "...                                                    ...   \n",
      "5116173                         Northeast Blackout of 2003   \n",
      "5116174  PieChartMaster- Unlock your Pie/Rose chart cre...   \n",
      "5116175                  The Greatest Checkmate Ever Given   \n",
      "5116176  Scrapling: Fast, Adaptive Web Scraping for Python   \n",
      "5116177                       Monkeys Predict US Elections   \n",
      "\n",
      "                                                       url  \\\n",
      "0        http://www.howtobearetronaut.com/wp-content/up...   \n",
      "1        http://blogs.lessthandot.com/index.php/WebDev/...   \n",
      "2        http://www.marketwatch.com/story/the-bay-citiz...   \n",
      "3        http://bits.blogs.nytimes.com/2013/07/15/why-w...   \n",
      "4                     http://paulgraham.com/bossnotes.html   \n",
      "...                                                    ...   \n",
      "5116173  https://en.wikipedia.org/wiki/Northeast_blacko...   \n",
      "5116174  https://apps.apple.com/us/app/piechartmaster-u...   \n",
      "5116175        https://www.youtube.com/watch?v=UULlFap1Zko   \n",
      "5116176               https://github.com/D4Vinci/Scrapling   \n",
      "5116177  https://www.biorxiv.org/content/10.1101/2024.0...   \n",
      "\n",
      "                        time  score            datetime day_of_week  \\\n",
      "0        2011-10-24 16:27:00     19 2011-10-24 16:27:00      Monday   \n",
      "1        2012-01-23 11:39:25      1 2012-01-23 11:39:25      Monday   \n",
      "2        2011-10-24 16:27:36      2 2011-10-24 16:27:36      Monday   \n",
      "3        2013-07-16 05:16:26      1 2013-07-16 05:16:26     Tuesday   \n",
      "4        2008-03-30 09:46:25      1 2008-03-30 09:46:25      Sunday   \n",
      "...                      ...    ...                 ...         ...   \n",
      "5116173  2024-10-13 23:41:46      1 2024-10-13 23:41:46      Sunday   \n",
      "5116174  2024-10-13 23:42:21      1 2024-10-13 23:42:21      Sunday   \n",
      "5116175  2024-10-13 23:45:21      1 2024-10-13 23:45:21      Sunday   \n",
      "5116176  2024-10-13 23:49:42      1 2024-10-13 23:49:42      Sunday   \n",
      "5116177  2024-10-13 23:53:00      1 2024-10-13 23:53:00      Sunday   \n",
      "\n",
      "         day_of_week_num    month  month_num  hour  year  \n",
      "0                      0  October         10    16  2011  \n",
      "1                      0  January          1    11  2012  \n",
      "2                      0  October         10    16  2011  \n",
      "3                      1     July          7     5  2013  \n",
      "4                      6    March          3     9  2008  \n",
      "...                  ...      ...        ...   ...   ...  \n",
      "5116173                6  October         10    23  2024  \n",
      "5116174                6  October         10    23  2024  \n",
      "5116175                6  October         10    23  2024  \n",
      "5116176                6  October         10    23  2024  \n",
      "5116177                6  October         10    23  2024  \n",
      "\n",
      "[4603891 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eac54ae-e678-4ba5-b8f2-c4b7c95a6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-13 23:53:00\n"
     ]
    }
   ],
   "source": [
    "max_time = df['datetime'].max()\n",
    "print(max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a6eb87-f621-4557-ae6c-1ba2265a1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_since_post1'] = max_time - df['datetime']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": 30,
>>>>>>> Stashed changes
   "id": "38c3c575-5441-45a6-9aea-4597e7699695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         4738 days 07:26:00\n",
      "1         4647 days 12:13:35\n",
      "2         4738 days 07:25:24\n",
      "3         4107 days 18:36:34\n",
      "4         6041 days 14:06:35\n",
      "                 ...        \n",
      "5116173      0 days 00:11:14\n",
      "5116174      0 days 00:10:39\n",
      "5116175      0 days 00:07:39\n",
      "5116176      0 days 00:03:18\n",
      "5116177      0 days 00:00:00\n",
      "Name: time_since_post1, Length: 4603891, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7a59f6-d5e1-4815-ad20-00378a9781cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Switched to eval mode. Pre-warming cache... ---\n",
      "--- Calculating and caching normalized embeddings... ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (in_embeddings): Embedding(71290, 50)\n",
       "  (out_embeddings): Linear(in_features=50, out_features=71290, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib8 import Word2Vec\n",
    "import torch\n",
    "\n",
    "embedding_dim = 50\n",
    "device_ids = [0, 1] # Use the first two GPUs\n",
    "device = torch.device(f\"cuda:{device_ids[0]}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Initialize Model, Loss, and Optimizer ---\n",
    "MODEL_PATH = f\"w2v/word2vec_pytorch_{embedding_dim}.pth\"\n",
    "\n",
    "model = Word2Vec.load_from_checkpoint(MODEL_PATH, device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3560e65a-3c01-4719-8df3-cdbf8155c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using cached embeddings. ---\n",
      "tensor([ 0.0151, -0.2932,  0.0731, -0.0389, -0.0133,  0.0778,  0.2062,  0.0685,\n",
      "         0.1959,  0.2224,  0.2976,  0.0808,  0.0589, -0.2884,  0.0526, -0.1409,\n",
      "        -0.0912, -0.1140, -0.1360, -0.2357,  0.3193, -0.0581,  0.0759, -0.0365,\n",
      "        -0.1061, -0.0400,  0.1124, -0.1369, -0.0608, -0.0191, -0.1755,  0.2196,\n",
      "         0.1463, -0.1008, -0.0974,  0.0120, -0.1018,  0.0551, -0.0144,  0.1746,\n",
      "         0.0509, -0.1130, -0.2061, -0.0268,  0.1000, -0.1205, -0.1274,  0.0267,\n",
      "         0.2003, -0.0287])\n",
      "--- Using cached embeddings. ---\n",
      "------------------------------\n",
      "is                   0.7491\n",
      "alive                0.6933\n",
      "was                  0.5683\n",
      "punchline            0.5467\n",
      "happens              0.5366\n",
      "isn                  0.5364\n",
      "stands               0.5295\n",
      "lop                  0.5261\n",
      "makes                0.5194\n",
      "distasteful          0.5169\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.find_most_similar('germany', top_n=10)\n",
    "# similar_words = model.find_most_similar('computer', top_n=10)\n",
    "\n",
    "# p = model.get_vector('paris')\n",
    "# f = model.get_vector('france')\n",
    "# g = model.get_vector('germany')\n",
    "\n",
    "f = model.get_sentence_vector('Elon Musk is alive')\n",
    "\n",
    "# bgf = p - f + g\n",
    "print(f)\n",
    "similar_words = model.find_most_similar_by_vector(f, top_n=10)\n",
    "\n",
    "if similar_words:\n",
    "    print(\"-\" * 30)\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word:<20} {score:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a2df4b-05ce-47b6-b453-8c456cc4ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "2. Time since the post was posted\n",
    "'''\n",
    "\n",
    "df['time_since_post'] = df['time_since_post1'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d564d13e-0814-47d8-8cc3-54986dfb71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          409389960.0\n",
      "1          401544815.0\n",
      "2          409389924.0\n",
      "3          354911794.0\n",
      "4          521993195.0\n",
      "              ...     \n",
      "5116173          674.0\n",
      "5116174          639.0\n",
      "5116175          459.0\n",
      "5116176          198.0\n",
      "5116177            0.0\n",
      "Name: time_since_post, Length: 4603891, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d433c7b-6ed7-40e3-a16e-ea54892c6348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing URLs: 100%|█████████████████| 4603891/4603891 [12:11<00:00, 6292.26it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. URL\n",
    "2. Domain\n",
    "'''\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Parsing URLs\")\n",
    "\n",
    "# Define a safe parser\n",
    "def safe_urlparse(url):\n",
    "    try: \n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc, parsed.path\n",
    "    except Exception:\n",
    "        return '', ''\n",
    "\n",
    "# Ensure URL column is clean\n",
    "df['url'] = df['url'].fillna('').astype(str)\n",
    "\n",
    "# Apply with progress bar\n",
    "df[['domain_name', 'url_path']] = df['url'].progress_apply(\n",
    "    lambda u: pd.Series(safe_urlparse(u))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "813127ec-43e3-4fc5-8d2c-29460b123364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chatgpt.com', '/c/684813b3-5bfc-8013-8f49e')\n"
     ]
    }
   ],
   "source": [
    "print(safe_urlparse('https://chatgpt.com/c/684813b3-5bfc-8013-8f49e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83995136-b25a-4ae1-b361-22772f061673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          http://www.howtobearetronaut.com/wp-content/up...\n",
      "1          http://blogs.lessthandot.com/index.php/WebDev/...\n",
      "2          http://www.marketwatch.com/story/the-bay-citiz...\n",
      "3          http://bits.blogs.nytimes.com/2013/07/15/why-w...\n",
      "4                       http://paulgraham.com/bossnotes.html\n",
      "                                 ...                        \n",
      "5116173    https://en.wikipedia.org/wiki/Northeast_blacko...\n",
      "5116174    https://apps.apple.com/us/app/piechartmaster-u...\n",
      "5116175          https://www.youtube.com/watch?v=UULlFap1Zko\n",
      "5116176                 https://github.com/D4Vinci/Scrapling\n",
      "5116177    https://www.biorxiv.org/content/10.1101/2024.0...\n",
      "Name: url, Length: 4603891, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b474c7dc-7120-4bd0-a210-3637e14ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. User name\n",
    "2. Title\n",
    "3. Length of Title\n",
    "4. Number of Upvotes\n",
    "'''\n",
    "\n",
    "df['by'] = df['by'].fillna('').astype(str)\n",
    "df['title'] = df['title'].fillna('').astype(str)\n",
    "df['title_length_chars'] = df['title'].str.len()\n",
    "df['title_length_words'] = df['title'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e63fec9-53e1-43f8-971d-e7e7e48275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting a file that has:\n",
    "- User ('by')\n",
    "- Title ('title')\n",
    "- Domain ('domain_name')\n",
    "- Day of the week ('day_of_week_num')\n",
    "- Month ('month')\n",
    "- Hour ('hour')\n",
    "- Year ('year')\n",
    "- Time since post ('time_since_post')\n",
    "- Title length chars ('title_length_chars')\n",
    "- Score ('score')\n",
    "'''\n",
    "\n",
    "selected_columns = ['by', 'title', 'domain_name', 'day_of_week_num', 'month_num', 'hour', 'year', 'time_since_post', 'title_length_chars', 'score']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected.to_csv('RelevantDataScrape.csv', index=False)\n",
    "\n",
    "df['domain_name'].value_counts()\n",
    "domain_name_counts = df['domain_name'].value_counts()\n",
    "df['by'].value_counts()\n",
    "username_counts = df['by'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65c7fff1-ae98-440c-b6ee-d1fb55e61d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "count\n"
     ]
    }
   ],
   "source": [
    "domaincolumn_names = domain_name_counts.name\n",
    "print(domaincolumn_names)\n",
    "usernamecolumn_names = username_counts.name\n",
    "print(usernamecolumn_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b25468a-d842-425b-af45-2d7c3236186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name\n",
      "github.com             160136\n",
      "medium.com             119463\n",
      "www.youtube.com        118512\n",
      "www.nytimes.com         70045\n",
      "techcrunch.com          50556\n",
      "                        ...  \n",
      "journals.plos.org        1014\n",
      "readwrite.com            1012\n",
      "foreignpolicy.com        1010\n",
      "www.sciencenews.org      1010\n",
      "t.co                     1006\n",
      "Name: count, Length: 319, dtype: int64\n",
      "by\n",
      "rbanffy           30512\n",
      "Tomte             23405\n",
      "tosh              20722\n",
      "pseudolus         16845\n",
      "bookofjoe         15801\n",
      "                  ...  \n",
      "dwynings           1018\n",
      "kristianp          1014\n",
      "ekianjo            1008\n",
      "nickfrost          1006\n",
      "ChrisArchitect     1005\n",
      "Name: count, Length: 352, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cutoff_value =  1000\n",
    "filtered_domain_counts = domain_name_counts[domain_name_counts > cutoff_value]\n",
    "print(filtered_domain_counts)\n",
    "\n",
    "filtered_username_counts = username_counts[username_counts > cutoff_value]\n",
    "print(filtered_username_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f7b224f-4b31-45dd-ab10-2213f8ad604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # This is where torch is imported\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "084532f7-67b1-4a5f-b007-04505f4537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RelevantDataScrape.csv', low_memory = False)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ec72bb9-866c-4948-80dd-ece93c462aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sentence embeddings: 100%|█| 4603771/4603771 [11:57<00:00, 6413.74it/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] title_embeddings.shape = (4603771, 50)\n"
     ]
    }
   ],
   "source": [
    "sentence_vectors = []\n",
    "for title in tqdm(df['title'], desc=\"Generating sentence embeddings\"):\n",
    "    with torch.no_grad():\n",
    "        vector = model.get_sentence_vector(str(title)).cpu().numpy()  # ensure numpy\n",
    "        sentence_vectors.append(vector)\n",
    "\n",
    "# Stack into a matrix (shape: num_samples x embedding_dim)\n",
    "title_embeddings = np.vstack(sentence_vectors)\n",
    "print(f\"[DEBUG] title_embeddings.shape = {title_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb212af-3a6e-4a3d-a766-02697e15c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[DEBUG] Shape of numerical_features: (4603771, 6)\n",
      "[DEBUG] Identified top 100 users for one-hot encoding.\n",
      "[DEBUG] User feature columns: ['user_Anon84', 'user_BerislavLopac', 'user_Brajeshwar', 'user_CapitalistCartr', 'user_ColinWright', 'user_CrankyBear', 'user_DanielRibeiro', 'user_DiabloD3', 'user_DyslexicAtheist', 'user_Garbage', 'user_JumpCrisscross', 'user_LinuxBender', 'user_OTHER', 'user_PaulHoule', 'user_Tomte', 'user_aaronbrethorst', 'user_adamnemecek', 'user_adrian_mrd', 'user_anigbrowl', 'user_based2', 'user_belter', 'user_bookofjoe', 'user_bootload', 'user_bryanrasmussen', 'user_clouddrover', 'user_colinprince', 'user_coloneltcb', 'user_cwan', 'user_danso', 'user_dnetesn', 'user_doener', 'user_dsr12', 'user_edw519', 'user_edward', 'user_elorant', 'user_elsewhen', 'user_evo_9', 'user_fanf2', 'user_feross', 'user_fortran77', 'user_geox', 'user_ghosh', 'user_giuliomagnifico', 'user_gk1', 'user_gmays', 'user_happy-go-lucky', 'user_headalgorithm', 'user_hhs', 'user_howard941', 'user_iProject', 'user_iafrikan', 'user_ilamont', 'user_ingve', 'user_jgrahamc', 'user_jkuria', 'user_joeyespo', 'user_jonbaer', 'user_jseliger', 'user_kiyanwang', 'user_laurex', 'user_lelf', 'user_luu', 'user_mfiguiere', 'user_mhb', 'user_mikece', 'user_mooreds', 'user_mpweiher', 'user_nickb', 'user_nreece', 'user_ohjeez', 'user_okket', 'user_pabs3', 'user_paulpauper', 'user_petethomas', 'user_prostoalex', 'user_protomyth', 'user_pseudolus', 'user_rbanffy', 'user_rmason', 'user_rntn', 'user_robg', 'user_samizdis', 'user_shawndumas', 'user_signa11', 'user_simonebrunozzi', 'user_smacktoward', 'user_sohkamyung', 'user_teleforce', 'user_thunderbong', 'user_tilt', 'user_todsacerdoti', 'user_tokenadult', 'user_tosh', 'user_uptown', 'user_vinnyglennon', 'user_wallflower', 'user_walterbell', 'user_wglb', 'user_wslh', 'user_yarapavan', 'user_zdw']\n",
      "[DEBUG] Shape of user_one_hot_features: (4603771, 101)\n",
      "[DEBUG] Identified top 100 domains for one-hot encoding.\n",
      "[DEBUG] Domain feature columns: ['domain_OTHER', 'domain_aeon.co', 'domain_apnews.com', 'domain_arstechnica.com', 'domain_arxiv.org', 'domain_aws.amazon.com', 'domain_bit.ly', 'domain_chrome.google.com', 'domain_dev.to', 'domain_docs.google.com', 'domain_edition.cnn.com', 'domain_en.wikipedia.org', 'domain_finance.yahoo.com', 'domain_fortune.com', 'domain_gigaom.com', 'domain_gist.github.com', 'domain_github.com', 'domain_gizmodo.com', 'domain_goo.gl', 'domain_hackaday.com', 'domain_hackernoon.com', 'domain_itunes.apple.com', 'domain_lwn.net', 'domain_mashable.com', 'domain_medium.com', 'domain_motherboard.vice.com', 'domain_nautil.us', 'domain_news.cnet.com', 'domain_news.ycombinator.com', 'domain_old.reddit.com', 'domain_online.wsj.com', 'domain_phys.org', 'domain_play.google.com', 'domain_qz.com', 'domain_spectrum.ieee.org', 'domain_stackoverflow.com', 'domain_techcrunch.com', 'domain_theconversation.com', 'domain_thenextweb.com', 'domain_torrentfreak.com', 'domain_twitter.com', 'domain_venturebeat.com', 'domain_vimeo.com', 'domain_www.axios.com', 'domain_www.bbc.co.uk', 'domain_www.bbc.com', 'domain_www.bloomberg.com', 'domain_www.businessinsider.com', 'domain_www.cbc.ca', 'domain_www.cnbc.com', 'domain_www.cnet.com', 'domain_www.cnn.com', 'domain_www.economist.com', 'domain_www.eff.org', 'domain_www.engadget.com', 'domain_www.facebook.com', 'domain_www.fastcompany.com', 'domain_www.forbes.com', 'domain_www.ft.com', 'domain_www.google.com', 'domain_www.guardian.co.uk', 'domain_www.huffingtonpost.com', 'domain_www.iafrikan.com', 'domain_www.independent.co.uk', 'domain_www.infoq.com', 'domain_www.kickstarter.com', 'domain_www.latimes.com', 'domain_www.linkedin.com', 'domain_www.macrumors.com', 'domain_www.nature.com', 'domain_www.newscientist.com', 'domain_www.newyorker.com', 'domain_www.npr.org', 'domain_www.nytimes.com', 'domain_www.phoronix.com', 'domain_www.quantamagazine.org', 'domain_www.quora.com', 'domain_www.readwriteweb.com', 'domain_www.reddit.com', 'domain_www.reuters.com', 'domain_www.sciencedaily.com', 'domain_www.scientificamerican.com', 'domain_www.sfgate.com', 'domain_www.slate.com', 'domain_www.slideshare.net', 'domain_www.techcrunch.com', 'domain_www.technologyreview.com', 'domain_www.telegraph.co.uk', 'domain_www.theatlantic.com', 'domain_www.theguardian.com', 'domain_www.theregister.co.uk', 'domain_www.theregister.com', 'domain_www.theverge.com', 'domain_www.vice.com', 'domain_www.vox.com', 'domain_www.washingtonpost.com', 'domain_www.wired.com', 'domain_www.wsj.com', 'domain_www.youtube.com', 'domain_www.zdnet.com', 'domain_youtu.be']\n",
      "[DEBUG] Shape of domain_one_hot_features: (4603771, 101)\n",
      "[DEBUG] Shape of target variable (y): (4603771, 1)\n",
      "\n",
      "PyTorch Neural Network Model Defined:\n",
      "HackerNewsPredictor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=258, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Loss function (MSE) and Optimizer (Adam) defined.\n",
      "\n",
      "Starting model training...\n",
      "Epoch [1/100], Train Loss: 3538.5042, Val Loss: 3518.8019\n",
      "Epoch [2/100], Train Loss: 3530.7089, Val Loss: 3509.0600\n",
      "Epoch [3/100], Train Loss: 3529.1213, Val Loss: 3509.7321\n",
      "Epoch [4/100], Train Loss: 3529.1691, Val Loss: 3508.4304\n",
      "Epoch [5/100], Train Loss: 3529.4222, Val Loss: 3511.0494\n",
      "Epoch [6/100], Train Loss: 3529.8232, Val Loss: 3508.3284\n",
      "Epoch [7/100], Train Loss: 3529.5480, Val Loss: 3505.9135\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "features = df[[\n",
    "    'score',\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars'\n",
    "]].values\n",
    "\n",
    "numerical_features = df[[\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars']].values\n",
    "print(f\"[DEBUG] Shape of numerical_features: {numerical_features.shape}\")\n",
    "\n",
    "if 'by' in df.columns:\n",
    "    num_top_users = 100\n",
    "    top_users = df['by'].value_counts().nlargest(num_top_users).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_users)} users for one-hot encoding.\")\n",
    "\n",
    "    # Create 'user_group' column with top users or 'OTHER'\n",
    "    df['user_group'] = df['by'].apply(lambda x: x if x in top_users else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    user_one_hot_features = pd.get_dummies(df['user_group'], prefix='user')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] User feature columns: {user_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of user_one_hot_features: {user_one_hot_features.shape}\")\n",
    "\n",
    "    user_one_hot_features_array = user_one_hot_features.values\n",
    "else:\n",
    "    user_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'by' column not available, skipping user one-hot encoding.\")\n",
    "\n",
    "# --- DOMAINS ---\n",
    "\n",
    "if 'domain_name' in df.columns:\n",
    "    num_top_domain = 100\n",
    "    top_domain = df['domain_name'].value_counts().nlargest(num_top_domain).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_domain)} domains for one-hot encoding.\")\n",
    "\n",
    "    # Create 'domain_group' column with top domains or 'OTHER'\n",
    "    df['domain_group'] = df['domain_name'].apply(lambda x: x if x in top_domain else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    domain_one_hot_features = pd.get_dummies(df['domain_group'], prefix='domain')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] Domain feature columns: {domain_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of domain_one_hot_features: {domain_one_hot_features.shape}\")\n",
    "\n",
    "    domain_one_hot_features_array = domain_one_hot_features.values\n",
    "else:\n",
    "    domain_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'domain_name' column not available, skipping domain one-hot encoding.\")\n",
    "\n",
    "# Combine features and target\n",
    "X = np.hstack((numerical_features, user_one_hot_features_array, domain_one_hot_features_array, title_embeddings))\n",
    "y = df[['score']].values.reshape(-1, 1)\n",
    "print(f\"[DEBUG] Shape of target variable (y): {y.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Shuffle and manually split 80/20\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "split = int(num_samples * 0.8)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "X_train_tensor = X_tensor[train_indices]\n",
    "y_train_tensor = y_tensor[train_indices]\n",
    "X_test_tensor = X_tensor[test_indices]\n",
    "y_test_tensor = y_tensor[test_indices]\n",
    "\n",
    "# Normalize using training set stats\n",
    "mean = X_train_tensor.mean(dim=0, keepdim=True)\n",
    "std = X_train_tensor.std(dim=0, keepdim=True)\n",
    "std[std == 0] = 1.0  # Prevent division by zero\n",
    "\n",
    "X_train_tensor = (X_train_tensor - mean) / std\n",
    "X_test_tensor = (X_test_tensor - mean) / std  # Use train set stats\n",
    "\n",
    "# Move to device (CPU or GPU)\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- 5. Define the Neural Network Model in PyTorch ---\n",
    "# Refactored to use nn.Sequential\n",
    "class HackerNewsPredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HackerNewsPredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(), # Activation function for the first hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), # Activation function for the second hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1) # Output layer with linear activation (default for nn.Linear)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = HackerNewsPredictor(input_dim).to(device) # Move model to device (CPU/GPU)\n",
    "\n",
    "print(\"\\nPyTorch Neural Network Model Defined:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# --- 6. Define Loss Function and Optimizer ---\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer with learning rate\n",
    "\n",
    "print(\"\\nLoss function (MSE) and Optimizer (Adam) defined.\")\n",
    "\n",
    "\n",
    "# --- 7. Train the Model ---\n",
    "num_epochs = 100 # Max epochs, EarlyStopping will manage it\n",
    "patience = 50 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # Move data to device\n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, targets) # Calculate loss\n",
    "        loss.backward() # Backward pass (compute gradients)\n",
    "        optimizer.step() # Update weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_actuals = []\n",
    "    with torch.no_grad(): # Disable gradient calculations during validation\n",
    "        for inputs, targets in test_loader: # Using test_loader for simplicity in this example for validation\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            val_running_loss += val_loss.item() * inputs.size(0)\n",
    "            val_predictions.extend(outputs.cpu().numpy())\n",
    "            val_actuals.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_dataset) # Use test_dataset for size calculation here\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping check\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth') # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "print(\"Model training finished.\")\n",
    "\n",
    "\n",
    "# --- 8. Evaluate the Model on the Test Set (or load best model and evaluate) ---\n",
    "model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_actuals.append(targets)\n",
    "\n",
    "# Concatenate tensors\n",
    "all_predictions = torch.cat(all_predictions).flatten()\n",
    "all_actuals = torch.cat(all_actuals).flatten()\n",
    "\n",
    "# Compute metrics using PyTorch only\n",
    "mse = torch.mean((all_predictions - all_actuals) ** 2).item()\n",
    "mae = torch.mean(torch.abs(all_predictions - all_actuals)).item()\n",
    "\n",
    "# R² calculation: 1 - SSR/SST\n",
    "ss_res = torch.sum((all_predictions - all_actuals) ** 2)\n",
    "ss_tot = torch.sum((all_actuals - torch.mean(all_actuals)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot).item()\n",
    "\n",
    "print(f\"\\nModel Evaluation on Test Set:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# --- 9. Make Predictions (Optional) ---\n",
    "# Prepare a sample for prediction (e.g., the first sample from the test set)\n",
    "sample_index = 0\n",
    "sample_input_scaled = X_test_tensor[sample_index:sample_index+1]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_upvotes_tensor = model(sample_input_scaled)\n",
    "    predicted_upvotes = predicted_upvotes_tensor.item()  # Convert to scalar\n",
    "\n",
    "# Actual value (assuming y_test is a PyTorch tensor or converted here)\n",
    "actual_upvotes = y_test_tensor[sample_index].item()\n",
    "\n",
    "print(f\"\\nPrediction for sample {sample_index}:\")\n",
    "print(f\"Actual Upvotes: {actual_upvotes}\")\n",
    "print(f\"Predicted Upvotes: {predicted_upvotes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd51c3f-530b-4a65-8e2a-616ea607500c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
