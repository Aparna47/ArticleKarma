{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7385ecb-6a10-45a7-8fcf-37463ddeb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # This is where torch is imported\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"~/Documents/Inputs1stGen.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f066768-e4be-4c6b-b2ef-5a5e11cf78a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'karma', 'title', 'url', 'time', 'score'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c577c4c2-baa0-4743-ae47-d5404b4be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "'''\n",
    "The following section gives us all the parameters relating to time:\n",
    "1. Day of week\n",
    "2. Time of day\n",
    "3. Month of Year\n",
    "4. Year of past \n",
    "'''\n",
    "\n",
    "#Converting UNIX timestamp to datetime\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "\n",
    "#Getting the names of the days of the week\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "\n",
    "#Creating a key of days of the week to numbers\n",
    "day_map = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5, 'Sunday':6\n",
    "}\n",
    "\n",
    "#Assigning numbers to the days of the week corresponding to the dataset\n",
    "df[\"day_of_week_num\"] = df['day_of_week'].map(day_map)\n",
    "\n",
    "#Getting the names of the months of year\n",
    "df['month'] = df['datetime'].dt.month_name()\n",
    "\n",
    "#Creating a key of months of the year to numbers\n",
    "month_map = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April':4, 'May': 5, 'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October':10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "df['month_num'] = df['month'].map(month_map)\n",
    "\n",
    "#Getting the hour of the post\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "#Getting the year of the post\n",
    "df['year'] = df['datetime'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26532795-6b37-4dd8-8bdd-a6e78cb43e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'karma', 'title', 'url', 'time', 'score', 'datetime',\n",
       "       'day_of_week', 'day_of_week_num', 'month', 'month_num', 'hour', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eac54ae-e678-4ba5-b8f2-c4b7c95a6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-14 00:00:50\n"
     ]
    }
   ],
   "source": [
    "max_time = df['datetime'].max()\n",
    "print(max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50a6eb87-f621-4557-ae6c-1ba2265a1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_since_post1'] = max_time - df['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a37c39c-d2c9-4590-ab30-06fa275b77a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'karma', 'title', 'url', 'time', 'score', 'datetime',\n",
       "       'day_of_week', 'day_of_week_num', 'month', 'month_num', 'hour', 'year',\n",
       "       'time_since_post1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38c3c575-5441-45a6-9aea-4597e7699695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         4738 days 07:33:50\n",
       "1         4647 days 12:21:25\n",
       "2         4738 days 07:33:14\n",
       "3         4107 days 18:44:24\n",
       "4         6041 days 14:14:25\n",
       "                 ...        \n",
       "5116176      0 days 00:11:08\n",
       "5116177      0 days 00:07:50\n",
       "5116178      0 days 00:03:39\n",
       "5116179      0 days 00:02:32\n",
       "5116180      0 days 00:00:00\n",
       "Name: time_since_post1, Length: 5116181, dtype: timedelta64[ns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['time_since_post1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0a2df4b-05ce-47b6-b453-8c456cc4ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "2. Time since the post was posted\n",
    "'''\n",
    "\n",
    "df['time_since_post'] = df['time_since_post1'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d564d13e-0814-47d8-8cc3-54986dfb71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          409390430.0\n",
      "1          401545285.0\n",
      "2          409390394.0\n",
      "3          354912264.0\n",
      "4          521993665.0\n",
      "              ...     \n",
      "5116176          668.0\n",
      "5116177          470.0\n",
      "5116178          219.0\n",
      "5116179          152.0\n",
      "5116180            0.0\n",
      "Name: time_since_post, Length: 5116181, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56ae8104-7163-460d-a3ec-4a7d42b54cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'karma', 'title', 'url', 'time', 'score', 'datetime',\n",
       "       'day_of_week', 'day_of_week_num', 'month', 'month_num', 'hour', 'year',\n",
       "       'time_since_post1', 'time_since_post'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d433c7b-6ed7-40e3-a16e-ea54892c6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. URL\n",
    "2. Domain\n",
    "'''\n",
    "\n",
    "# Define a safe parser\n",
    "def safe_urlparse(url):\n",
    "    try: \n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc, parsed.path\n",
    "    except Exception:\n",
    "        return '', ''\n",
    "\n",
    "# Apply safely\n",
    "df['url'] = df['url'].fillna('').astype(str)  # Ensure it's string\n",
    "df[['domain_name', 'url_path']] = df['url'].apply(\n",
    "    lambda u: pd.Series(safe_urlparse(u))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a5cf115-111c-40b5-984e-5adbc5ab8421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['by', 'karma', 'title', 'url', 'time', 'score', 'datetime',\n",
       "       'day_of_week', 'day_of_week_num', 'month', 'month_num', 'hour', 'year',\n",
       "       'time_since_post1', 'time_since_post', 'domain_name', 'url_path'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b474c7dc-7120-4bd0-a210-3637e14ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. User name\n",
    "2. Title\n",
    "3. Length of Title\n",
    "4. Number of Upvotes\n",
    "'''\n",
    "\n",
    "df['by'] = df['by'].fillna('').astype(str)\n",
    "df['title'] = df['title'].fillna('').astype(str)\n",
    "df['title_length_chars'] = df['title'].str.len()\n",
    "df['title_length_words'] = df['title'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e63fec9-53e1-43f8-971d-e7e7e48275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting a file that has:\n",
    "- User ('by')\n",
    "- Title ('title')\n",
    "- Domain ('domain_name')\n",
    "- Day of the week ('day_of_week_num')\n",
    "- Month ('month')\n",
    "- Hour ('hour')\n",
    "- Year ('year')\n",
    "- Time since post ('time_since_post')\n",
    "- Title length chars ('title_length_chars')\n",
    "- Score ('score')\n",
    "'''\n",
    "\n",
    "selected_columns = ['by', 'title', 'domain_name', 'day_of_week_num', 'month_num', 'hour', 'year', 'time_since_post', 'title_length_chars', 'score']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected.to_csv('RelevantDataScrape.csv', index=False)\n",
    "\n",
    "df['domain_name'].value_counts()\n",
    "domain_name_counts = df['domain_name'].value_counts()\n",
    "df['by'].value_counts()\n",
    "username_counts = df['by'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4f286bf-6d9a-41fc-82bd-34856cf7bb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain_name\n",
       "                           512396\n",
       "github.com                 160136\n",
       "medium.com                 119464\n",
       "www.youtube.com            118512\n",
       "www.nytimes.com             70045\n",
       "                            ...  \n",
       "www.cummins-engine.es           1\n",
       "yaketyhack.blogspot.com         1\n",
       "forum.thinkpads.com             1\n",
       "www.wb6nvh.com                  1\n",
       "yongebai.github.io              1\n",
       "Name: count, Length: 575001, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_name_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c40222a-9dd8-4e2c-8aac-e9581557f577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "by\n",
       "rbanffy         30849\n",
       "Tomte           23651\n",
       "tosh            20919\n",
       "pseudolus       16944\n",
       "bookofjoe       16097\n",
       "                ...  \n",
       "giladha             1\n",
       "skynetswed          1\n",
       "18567478            1\n",
       "sophxkath123        1\n",
       "partime             1\n",
       "Name: count, Length: 512588, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c7fff1-ae98-440c-b6ee-d1fb55e61d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "count\n"
     ]
    }
   ],
   "source": [
    "domaincolumn_names = domain_name_counts.name\n",
    "print(domaincolumn_names)\n",
    "usernamecolumn_names = username_counts.name\n",
    "print(usernamecolumn_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b25468a-d842-425b-af45-2d7c3236186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name\n",
      "                       512396\n",
      "github.com             160136\n",
      "medium.com             119464\n",
      "www.youtube.com        118512\n",
      "www.nytimes.com         70045\n",
      "                        ...  \n",
      "journals.plos.org        1014\n",
      "readwrite.com            1012\n",
      "www.sciencenews.org      1010\n",
      "foreignpolicy.com        1010\n",
      "t.co                     1006\n",
      "Name: count, Length: 320, dtype: int64\n",
      "by\n",
      "rbanffy           30849\n",
      "Tomte             23651\n",
      "tosh              20919\n",
      "pseudolus         16944\n",
      "bookofjoe         16097\n",
      "                  ...  \n",
      "Bostonian          1018\n",
      "ValentineC         1009\n",
      "abraham            1005\n",
      "danw               1005\n",
      "lyricsongation     1004\n",
      "Name: count, Length: 365, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cutoff_value =  1000\n",
    "filtered_domain_counts = domain_name_counts[domain_name_counts > cutoff_value]\n",
    "print(filtered_domain_counts)\n",
    "\n",
    "filtered_username_counts = username_counts[username_counts > cutoff_value]\n",
    "print(filtered_username_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50a28cc7-0016-4159-82d7-c73caceea2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d25a5fd-b207-4197-b3ce-53fb1c33c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deb212af-3a6e-4a3d-a766-02697e15c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('RelevantDataScrape.csv', low_memory = False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b1eef58-05b6-4913-a189-93835c77ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Shape of numerical_features: (5116181, 6)\n"
     ]
    }
   ],
   "source": [
    "features = df[[\n",
    "    'score',\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars'\n",
    "]].values\n",
    "\n",
    "numerical_features = df[[\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars']].values\n",
    "print(f\"[DEBUG] Shape of numerical_features: {numerical_features.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bf0b6c0-5166-44e1-85e5-4e4825d50a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Identified top 100 users for one-hot encoding.\n",
      "[DEBUG] User feature columns: ['user_Anon84', 'user_BerislavLopac', 'user_Brajeshwar', 'user_CapitalistCartr', 'user_ColinWright', 'user_CrankyBear', 'user_DanielRibeiro', 'user_DiabloD3', 'user_DyslexicAtheist', 'user_Garbage', 'user_JumpCrisscross', 'user_LinuxBender', 'user_OTHER', 'user_PaulHoule', 'user_Tomte', 'user_aaronbrethorst', 'user_adamnemecek', 'user_adrian_mrd', 'user_amichail', 'user_anigbrowl', 'user_based2', 'user_belter', 'user_bookofjoe', 'user_bootload', 'user_bryanrasmussen', 'user_colinprince', 'user_coloneltcb', 'user_cwan', 'user_danso', 'user_dnetesn', 'user_doener', 'user_dsr12', 'user_edw519', 'user_edward', 'user_elorant', 'user_elsewhen', 'user_evo_9', 'user_fanf2', 'user_feross', 'user_fortran77', 'user_geox', 'user_ghosh', 'user_giuliomagnifico', 'user_gk1', 'user_gmays', 'user_happy-go-lucky', 'user_headalgorithm', 'user_hhs', 'user_howard941', 'user_iProject', 'user_iafrikan', 'user_ilamont', 'user_ingve', 'user_jgrahamc', 'user_jkuria', 'user_joeyespo', 'user_jonbaer', 'user_jseliger', 'user_kiyanwang', 'user_laurex', 'user_lelf', 'user_luu', 'user_marban', 'user_mfiguiere', 'user_mhb', 'user_mikece', 'user_mooreds', 'user_mpweiher', 'user_nickb', 'user_nreece', 'user_ohjeez', 'user_okket', 'user_pabs3', 'user_paulpauper', 'user_petethomas', 'user_prostoalex', 'user_protomyth', 'user_pseudolus', 'user_rbanffy', 'user_rmason', 'user_rntn', 'user_robg', 'user_samizdis', 'user_shawndumas', 'user_signa11', 'user_simonebrunozzi', 'user_smacktoward', 'user_sohkamyung', 'user_teleforce', 'user_thunderbong', 'user_todsacerdoti', 'user_tokenadult', 'user_tosh', 'user_uptown', 'user_vinnyglennon', 'user_wallflower', 'user_walterbell', 'user_wglb', 'user_wslh', 'user_yarapavan', 'user_zdw']\n",
      "[DEBUG] Shape of user_one_hot_features: (5116181, 101)\n"
     ]
    }
   ],
   "source": [
    "# --- USERS (i.e. by) ---\n",
    "\n",
    "if 'by' in df.columns:\n",
    "    num_top_users = 100\n",
    "    top_users = df['by'].value_counts().nlargest(num_top_users).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_users)} users for one-hot encoding.\")\n",
    "\n",
    "    # Create 'user_group' column with top users or 'OTHER'\n",
    "    df['user_group'] = df['by'].apply(lambda x: x if x in top_users else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    user_one_hot_features = pd.get_dummies(df['user_group'], prefix='user')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] User feature columns: {user_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of user_one_hot_features: {user_one_hot_features.shape}\")\n",
    "\n",
    "    user_one_hot_features_array = user_one_hot_features.values\n",
    "else:\n",
    "    user_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'by' column not available, skipping user one-hot encoding.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baff2a9e-24c9-42da-8a9a-95324ffd45c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Identified top 100 domains for one-hot encoding.\n",
      "[DEBUG] Domain feature columns: ['domain_OTHER', 'domain_aeon.co', 'domain_apnews.com', 'domain_arstechnica.com', 'domain_arxiv.org', 'domain_aws.amazon.com', 'domain_bit.ly', 'domain_chrome.google.com', 'domain_dev.to', 'domain_docs.google.com', 'domain_edition.cnn.com', 'domain_en.wikipedia.org', 'domain_finance.yahoo.com', 'domain_fortune.com', 'domain_gigaom.com', 'domain_gist.github.com', 'domain_github.com', 'domain_gizmodo.com', 'domain_goo.gl', 'domain_hackaday.com', 'domain_hackernoon.com', 'domain_itunes.apple.com', 'domain_lwn.net', 'domain_mashable.com', 'domain_medium.com', 'domain_motherboard.vice.com', 'domain_nautil.us', 'domain_news.cnet.com', 'domain_news.ycombinator.com', 'domain_old.reddit.com', 'domain_online.wsj.com', 'domain_phys.org', 'domain_play.google.com', 'domain_qz.com', 'domain_spectrum.ieee.org', 'domain_stackoverflow.com', 'domain_techcrunch.com', 'domain_theconversation.com', 'domain_thenextweb.com', 'domain_torrentfreak.com', 'domain_twitter.com', 'domain_venturebeat.com', 'domain_vimeo.com', 'domain_www.axios.com', 'domain_www.bbc.co.uk', 'domain_www.bbc.com', 'domain_www.bloomberg.com', 'domain_www.businessinsider.com', 'domain_www.cbc.ca', 'domain_www.cnbc.com', 'domain_www.cnet.com', 'domain_www.cnn.com', 'domain_www.economist.com', 'domain_www.eff.org', 'domain_www.engadget.com', 'domain_www.facebook.com', 'domain_www.fastcompany.com', 'domain_www.forbes.com', 'domain_www.ft.com', 'domain_www.google.com', 'domain_www.guardian.co.uk', 'domain_www.huffingtonpost.com', 'domain_www.iafrikan.com', 'domain_www.independent.co.uk', 'domain_www.infoq.com', 'domain_www.kickstarter.com', 'domain_www.latimes.com', 'domain_www.linkedin.com', 'domain_www.macrumors.com', 'domain_www.nature.com', 'domain_www.newscientist.com', 'domain_www.newyorker.com', 'domain_www.npr.org', 'domain_www.nytimes.com', 'domain_www.phoronix.com', 'domain_www.quantamagazine.org', 'domain_www.quora.com', 'domain_www.readwriteweb.com', 'domain_www.reddit.com', 'domain_www.reuters.com', 'domain_www.sciencedaily.com', 'domain_www.scientificamerican.com', 'domain_www.sfgate.com', 'domain_www.slate.com', 'domain_www.slideshare.net', 'domain_www.techcrunch.com', 'domain_www.technologyreview.com', 'domain_www.telegraph.co.uk', 'domain_www.theatlantic.com', 'domain_www.theguardian.com', 'domain_www.theregister.co.uk', 'domain_www.theregister.com', 'domain_www.theverge.com', 'domain_www.vice.com', 'domain_www.vox.com', 'domain_www.washingtonpost.com', 'domain_www.wired.com', 'domain_www.wsj.com', 'domain_www.youtube.com', 'domain_www.zdnet.com', 'domain_youtu.be']\n",
      "[DEBUG] Shape of domain_one_hot_features: (5116181, 101)\n"
     ]
    }
   ],
   "source": [
    "# --- DOMAINS ---\n",
    "\n",
    "if 'domain_name' in df.columns:\n",
    "    num_top_domain = 100\n",
    "    top_domain = df['domain_name'].value_counts().nlargest(num_top_domain).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_domain)} domains for one-hot encoding.\")\n",
    "\n",
    "    # Create 'domain_group' column with top domains or 'OTHER'\n",
    "    df['domain_group'] = df['domain_name'].apply(lambda x: x if x in top_domain else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    domain_one_hot_features = pd.get_dummies(df['domain_group'], prefix='domain')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] Domain feature columns: {domain_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of domain_one_hot_features: {domain_one_hot_features.shape}\")\n",
    "\n",
    "    domain_one_hot_features_array = domain_one_hot_features.values\n",
    "else:\n",
    "    domain_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'domain_name' column not available, skipping domain one-hot encoding.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0c77962-a284-4682-a3f0-6d2345540579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Shape of target variable (y): (5116181, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack((numerical_features, user_one_hot_features_array, domain_one_hot_features_array))\n",
    "y = df[['score']].values.reshape(-1, 1)\n",
    "print(f\"[DEBUG] Shape of target variable (y): {y.shape}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4756c641-96f9-492a-9f60-89e977e98abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyTorch Neural Network Model Defined:\n",
      "HackerNewsPredictor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=208, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Define the Neural Network Model in PyTorch ---\n",
    "# Refactored to use nn.Sequential\n",
    "import os\n",
    "class HackerNewsPredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HackerNewsPredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(), # Activation function for the first hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), # Activation function for the second hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1) # Output layer with linear activation (default for nn.Linear)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = HackerNewsPredictor(input_dim).to(device) # Move model to device (CPU/GPU)\n",
    "if os.path.exists('best_model.pth'):\n",
    "   model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "\n",
    "print(\"\\nPyTorch Neural Network Model Defined:\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1251f49-6fb3-479a-a2c8-c03151eb1e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function (MSE) and Optimizer (Adam) defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Define Loss Function and Optimizer ---\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer with learning rate\n",
    "\n",
    "print(\"\\nLoss function (MSE) and Optimizer (Adam) defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7550699a-4276-4816-b9e7-6a8c656fb772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch [1/100], Train Loss: 3335.4574, Val Loss: 3366.8234\n",
      "Epoch [2/100], Train Loss: 3334.4240, Val Loss: 3367.4970\n",
      "Epoch [3/100], Train Loss: 3334.3548, Val Loss: 3365.6197\n",
      "Epoch [4/100], Train Loss: 3334.6003, Val Loss: 3366.7110\n",
      "Epoch [5/100], Train Loss: 3335.8338, Val Loss: 3367.9162\n",
      "Epoch [6/100], Train Loss: 3336.2692, Val Loss: 3367.5730\n",
      "Epoch [7/100], Train Loss: 3337.1862, Val Loss: 3368.8800\n",
      "Epoch [8/100], Train Loss: 3337.5078, Val Loss: 3367.5418\n",
      "Epoch [9/100], Train Loss: 3336.9953, Val Loss: 3367.8629\n",
      "Epoch [10/100], Train Loss: 3337.7119, Val Loss: 3367.3289\n",
      "Epoch [11/100], Train Loss: 3336.8226, Val Loss: 3369.9222\n",
      "Epoch [12/100], Train Loss: 3337.0052, Val Loss: 3365.9089\n",
      "Epoch [13/100], Train Loss: 3336.8986, Val Loss: 3370.6719\n",
      "Early stopping at epoch 13 as validation loss did not improve for 10 epochs.\n",
      "Model training finished.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Train the Model ---\n",
    "num_epochs = 100 # Max epochs, EarlyStopping will manage it\n",
    "patience = 10 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # Move data to device\n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, targets) # Calculate loss\n",
    "        loss.backward() # Backward pass (compute gradients)\n",
    "        optimizer.step() # Update weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_actuals = []\n",
    "    with torch.no_grad(): # Disable gradient calculations during validation\n",
    "        for inputs, targets in test_loader: # Using test_loader for simplicity in this example for validation\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            val_running_loss += val_loss.item() * inputs.size(0)\n",
    "            val_predictions.extend(outputs.cpu().numpy())\n",
    "            val_actuals.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_dataset) # Use test_dataset for size calculation here\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping check\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth') # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "print(\"Model training finished.\")\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model.pth') # Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df9aafe2-070e-437e-b566-f83309b8ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation on Test Set:\n",
      "Mean Squared Error (MSE): 3370.67\n",
      "Mean Absolute Error (MAE): 20.79\n",
      "R-squared (R²): 0.01\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Evaluate the Model on the Test Set (or load best model and evaluate) ---\n",
    "model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_actuals.append(targets)\n",
    "\n",
    "# Concatenate tensors\n",
    "all_predictions = torch.cat(all_predictions).flatten()\n",
    "all_actuals = torch.cat(all_actuals).flatten()\n",
    "\n",
    "# Compute metrics using PyTorch only\n",
    "mse = torch.mean((all_predictions - all_actuals) ** 2).item()\n",
    "mae = torch.mean(torch.abs(all_predictions - all_actuals)).item()\n",
    "\n",
    "# R² calculation: 1 - SSR/SST\n",
    "ss_res = torch.sum((all_predictions - all_actuals) ** 2)\n",
    "ss_tot = torch.sum((all_actuals - torch.mean(all_actuals)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot).item()\n",
    "\n",
    "print(f\"\\nModel Evaluation on Test Set:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "19e55bc6-becb-4e7e-bcb7-42488befb6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for sample 0:\n",
      "Actual Upvotes: 32\n",
      "Predicted Upvotes: 13.43\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Make Predictions (Optional) ---\n",
    "# Prepare a sample for prediction (e.g., the first sample from the test set)\n",
    "sample_index = 0\n",
    "sample_input_scaled = torch.tensor(X_test_scaled[sample_index:sample_index+1], dtype=torch.float32).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_upvotes_tensor = model(sample_input_scaled)\n",
    "    predicted_upvotes = predicted_upvotes_tensor.item()  # Convert to scalar\n",
    "\n",
    "# Actual value (assuming y_test is a PyTorch tensor or converted here)\n",
    "actual_upvotes = torch.tensor(y_test[sample_index][0]).item()\n",
    "\n",
    "print(f\"\\nPrediction for sample {sample_index}:\")\n",
    "print(f\"Actual Upvotes: {actual_upvotes}\")\n",
    "print(f\"Predicted Upvotes: {predicted_upvotes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2029d7e-e500-451a-a65e-022edb5b1895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
