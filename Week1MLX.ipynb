{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7385ecb-6a10-45a7-8fcf-37463ddeb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"~/documents/Inputs1stGen.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f066768-e4be-4c6b-b2ef-5a5e11cf78a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      by   karma  \\\n",
      "0            ColinWright  127765   \n",
      "1                   hncj      44   \n",
      "2             andymboyle     714   \n",
      "3                digisth    2395   \n",
      "4                jazzdev     543   \n",
      "...                  ...     ...   \n",
      "5116176          d4vinci       1   \n",
      "5116177  giuliomagnifico   40731   \n",
      "5116178    retronick2020      78   \n",
      "5116179   ElizabethMoore       1   \n",
      "5116180          partime       0   \n",
      "\n",
      "                                                     title  \\\n",
      "0        \"What May Happen in the Next Hundred Years\", f...   \n",
      "1             Getting Started with JavaScript Unit Testing   \n",
      "2        Armstrong, the Django-based and open-source ne...   \n",
      "3                     Why Web Reviewers Make Up Bad Things   \n",
      "4        You Weren't Meant to Have a Boss: The Cliff Notes   \n",
      "...                                                    ...   \n",
      "5116176  Scrapling: Fast, Adaptive Web Scraping for Python   \n",
      "5116177                       Monkeys Predict US Elections   \n",
      "5116178                                                NaN   \n",
      "5116179                                                NaN   \n",
      "5116180                                                NaN   \n",
      "\n",
      "                                                       url  \\\n",
      "0        http://www.howtobearetronaut.com/wp-content/up...   \n",
      "1        http://blogs.lessthandot.com/index.php/WebDev/...   \n",
      "2        http://www.marketwatch.com/story/the-bay-citiz...   \n",
      "3        http://bits.blogs.nytimes.com/2013/07/15/why-w...   \n",
      "4                     http://paulgraham.com/bossnotes.html   \n",
      "...                                                    ...   \n",
      "5116176               https://github.com/D4Vinci/Scrapling   \n",
      "5116177  https://www.biorxiv.org/content/10.1101/2024.0...   \n",
      "5116178                                                NaN   \n",
      "5116179                                                NaN   \n",
      "5116180                                                NaN   \n",
      "\n",
      "                        time  score  \n",
      "0        2011-10-24 16:27:00     19  \n",
      "1        2012-01-23 11:39:25      1  \n",
      "2        2011-10-24 16:27:36      2  \n",
      "3        2013-07-16 05:16:26      1  \n",
      "4        2008-03-30 09:46:25      1  \n",
      "...                      ...    ...  \n",
      "5116176  2024-10-13 23:49:42      1  \n",
      "5116177  2024-10-13 23:53:00      1  \n",
      "5116178  2024-10-13 23:57:11      1  \n",
      "5116179  2024-10-13 23:58:18      1  \n",
      "5116180  2024-10-14 00:00:50      1  \n",
      "\n",
      "[5116181 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c577c4c2-baa0-4743-ae47-d5404b4be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "'''\n",
    "The following section gives us all the parameters relating to time:\n",
    "1. Day of week\n",
    "2. Time of day\n",
    "3. Month of Year\n",
    "4. Year of past \n",
    "'''\n",
    "\n",
    "#Converting UNIX timestamp to datetime\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "\n",
    "#Getting the names of the days of the week\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "\n",
    "#Creating a key of days of the week to numbers\n",
    "day_map = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5, 'Sunday':6\n",
    "}\n",
    "\n",
    "#Assigning numbers to the days of the week corresponding to the dataset\n",
    "df[\"day_of_week_num\"] = df['day_of_week'].map(day_map)\n",
    "\n",
    "#Getting the names of the months of year\n",
    "df['month'] = df['datetime'].dt.month_name()\n",
    "\n",
    "#Creating a key of months of the year to numbers\n",
    "month_map = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April':4, 'May': 5, 'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October':10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "df['month_num'] = df['month'].map(month_map)\n",
    "\n",
    "#Getting the hour of the post\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "#Getting the year of the post\n",
    "df['year'] = df['datetime'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26532795-6b37-4dd8-8bdd-a6e78cb43e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      by   karma  \\\n",
      "0            ColinWright  127765   \n",
      "1                   hncj      44   \n",
      "2             andymboyle     714   \n",
      "3                digisth    2395   \n",
      "4                jazzdev     543   \n",
      "...                  ...     ...   \n",
      "5116176          d4vinci       1   \n",
      "5116177  giuliomagnifico   40731   \n",
      "5116178    retronick2020      78   \n",
      "5116179   ElizabethMoore       1   \n",
      "5116180          partime       0   \n",
      "\n",
      "                                                     title  \\\n",
      "0        \"What May Happen in the Next Hundred Years\", f...   \n",
      "1             Getting Started with JavaScript Unit Testing   \n",
      "2        Armstrong, the Django-based and open-source ne...   \n",
      "3                     Why Web Reviewers Make Up Bad Things   \n",
      "4        You Weren't Meant to Have a Boss: The Cliff Notes   \n",
      "...                                                    ...   \n",
      "5116176  Scrapling: Fast, Adaptive Web Scraping for Python   \n",
      "5116177                       Monkeys Predict US Elections   \n",
      "5116178                                                NaN   \n",
      "5116179                                                NaN   \n",
      "5116180                                                NaN   \n",
      "\n",
      "                                                       url  \\\n",
      "0        http://www.howtobearetronaut.com/wp-content/up...   \n",
      "1        http://blogs.lessthandot.com/index.php/WebDev/...   \n",
      "2        http://www.marketwatch.com/story/the-bay-citiz...   \n",
      "3        http://bits.blogs.nytimes.com/2013/07/15/why-w...   \n",
      "4                     http://paulgraham.com/bossnotes.html   \n",
      "...                                                    ...   \n",
      "5116176               https://github.com/D4Vinci/Scrapling   \n",
      "5116177  https://www.biorxiv.org/content/10.1101/2024.0...   \n",
      "5116178                                                NaN   \n",
      "5116179                                                NaN   \n",
      "5116180                                                NaN   \n",
      "\n",
      "                        time  score            datetime day_of_week  \\\n",
      "0        2011-10-24 16:27:00     19 2011-10-24 16:27:00      Monday   \n",
      "1        2012-01-23 11:39:25      1 2012-01-23 11:39:25      Monday   \n",
      "2        2011-10-24 16:27:36      2 2011-10-24 16:27:36      Monday   \n",
      "3        2013-07-16 05:16:26      1 2013-07-16 05:16:26     Tuesday   \n",
      "4        2008-03-30 09:46:25      1 2008-03-30 09:46:25      Sunday   \n",
      "...                      ...    ...                 ...         ...   \n",
      "5116176  2024-10-13 23:49:42      1 2024-10-13 23:49:42      Sunday   \n",
      "5116177  2024-10-13 23:53:00      1 2024-10-13 23:53:00      Sunday   \n",
      "5116178  2024-10-13 23:57:11      1 2024-10-13 23:57:11      Sunday   \n",
      "5116179  2024-10-13 23:58:18      1 2024-10-13 23:58:18      Sunday   \n",
      "5116180  2024-10-14 00:00:50      1 2024-10-14 00:00:50      Monday   \n",
      "\n",
      "         day_of_week_num    month  month_num  hour  year  \n",
      "0                      0  October         10    16  2011  \n",
      "1                      0  January          1    11  2012  \n",
      "2                      0  October         10    16  2011  \n",
      "3                      1     July          7     5  2013  \n",
      "4                      6    March          3     9  2008  \n",
      "...                  ...      ...        ...   ...   ...  \n",
      "5116176                6  October         10    23  2024  \n",
      "5116177                6  October         10    23  2024  \n",
      "5116178                6  October         10    23  2024  \n",
      "5116179                6  October         10    23  2024  \n",
      "5116180                0  October         10     0  2024  \n",
      "\n",
      "[5116181 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5eac54ae-e678-4ba5-b8f2-c4b7c95a6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-14 00:00:50\n"
     ]
    }
   ],
   "source": [
    "max_time = df['datetime'].max()\n",
    "print(max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50a6eb87-f621-4557-ae6c-1ba2265a1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_since_post1'] = max_time - df['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a37c39c-d2c9-4590-ab30-06fa275b77a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_since_post1\n"
     ]
    }
   ],
   "source": [
    "print('time_since_post1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38c3c575-5441-45a6-9aea-4597e7699695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         4738 days 07:33:50\n",
      "1         4647 days 12:21:25\n",
      "2         4738 days 07:33:14\n",
      "3         4107 days 18:44:24\n",
      "4         6041 days 14:14:25\n",
      "                 ...        \n",
      "5116176      0 days 00:11:08\n",
      "5116177      0 days 00:07:50\n",
      "5116178      0 days 00:03:39\n",
      "5116179      0 days 00:02:32\n",
      "5116180      0 days 00:00:00\n",
      "Name: time_since_post1, Length: 5116181, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0a2df4b-05ce-47b6-b453-8c456cc4ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "2. Time since the post was posted\n",
    "'''\n",
    "\n",
    "df['time_since_post'] = df['time_since_post1'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d564d13e-0814-47d8-8cc3-54986dfb71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          409390430.0\n",
      "1          401545285.0\n",
      "2          409390394.0\n",
      "3          354912264.0\n",
      "4          521993665.0\n",
      "              ...     \n",
      "5116176          668.0\n",
      "5116177          470.0\n",
      "5116178          219.0\n",
      "5116179          152.0\n",
      "5116180            0.0\n",
      "Name: time_since_post, Length: 5116181, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d433c7b-6ed7-40e3-a16e-ea54892c6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. URL\n",
    "2. Domain\n",
    "'''\n",
    "\n",
    "# Define a safe parser\n",
    "def safe_urlparse(url):\n",
    "    try: \n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc, parsed.path\n",
    "    except Exception:\n",
    "        return '', ''\n",
    "\n",
    "# Apply safely\n",
    "df['url'] = df['url'].fillna('').astype(str)  # Ensure it's string\n",
    "df[['domain_name', 'url_path']] = df['url'].apply(\n",
    "    lambda u: pd.Series(safe_urlparse(u))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "813127ec-43e3-4fc5-8d2c-29460b123364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chatgpt.com', '/c/684813b3-5bfc-8013-8f49e')\n"
     ]
    }
   ],
   "source": [
    "print(safe_urlparse('https://chatgpt.com/c/684813b3-5bfc-8013-8f49e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83995136-b25a-4ae1-b361-22772f061673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          http://www.howtobearetronaut.com/wp-content/up...\n",
      "1          http://blogs.lessthandot.com/index.php/WebDev/...\n",
      "2          http://www.marketwatch.com/story/the-bay-citiz...\n",
      "3          http://bits.blogs.nytimes.com/2013/07/15/why-w...\n",
      "4                       http://paulgraham.com/bossnotes.html\n",
      "                                 ...                        \n",
      "5116176                 https://github.com/D4Vinci/Scrapling\n",
      "5116177    https://www.biorxiv.org/content/10.1101/2024.0...\n",
      "5116178                                                     \n",
      "5116179                                                     \n",
      "5116180                                                     \n",
      "Name: url, Length: 5116181, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b474c7dc-7120-4bd0-a210-3637e14ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. User name\n",
    "2. Title\n",
    "3. Length of Title\n",
    "4. Number of Upvotes\n",
    "'''\n",
    "\n",
    "df['by'] = df['by'].fillna('').astype(str)\n",
    "df['title'] = df['title'].fillna('').astype(str)\n",
    "df['title_length_chars'] = df['title'].str.len()\n",
    "df['title_length_words'] = df['title'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e63fec9-53e1-43f8-971d-e7e7e48275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting a file that has:\n",
    "- User ('by')\n",
    "- Title ('title')\n",
    "- Domain ('domain_name')\n",
    "- Day of the week ('day_of_week_num')\n",
    "- Month ('month')\n",
    "- Hour ('hour')\n",
    "- Year ('year')\n",
    "- Time since post ('time_since_post')\n",
    "- Title length chars ('title_length_chars')\n",
    "- Score ('score')\n",
    "'''\n",
    "\n",
    "selected_columns = ['by', 'title', 'domain_name', 'day_of_week_num', 'month_num', 'hour', 'year', 'time_since_post', 'title_length_chars', 'score']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected.to_csv('RelevantDataScrape.csv', index=False)\n",
    "\n",
    "df['domain_name'].value_counts()\n",
    "domain_name_counts = df['domain_name'].value_counts()\n",
    "df['by'].value_counts()\n",
    "username_counts = df['by'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65c7fff1-ae98-440c-b6ee-d1fb55e61d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "count\n"
     ]
    }
   ],
   "source": [
    "domaincolumn_names = domain_name_counts.name\n",
    "print(domaincolumn_names)\n",
    "usernamecolumn_names = username_counts.name\n",
    "print(usernamecolumn_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b25468a-d842-425b-af45-2d7c3236186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name\n",
      "                       512396\n",
      "github.com             160136\n",
      "medium.com             119464\n",
      "www.youtube.com        118512\n",
      "www.nytimes.com         70045\n",
      "                        ...  \n",
      "journals.plos.org        1014\n",
      "readwrite.com            1012\n",
      "www.sciencenews.org      1010\n",
      "foreignpolicy.com        1010\n",
      "t.co                     1006\n",
      "Name: count, Length: 320, dtype: int64\n",
      "by\n",
      "rbanffy           30849\n",
      "Tomte             23651\n",
      "tosh              20919\n",
      "pseudolus         16944\n",
      "bookofjoe         16097\n",
      "                  ...  \n",
      "Bostonian          1018\n",
      "ValentineC         1009\n",
      "danw               1005\n",
      "abraham            1005\n",
      "lyricsongation     1004\n",
      "Name: count, Length: 365, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cutoff_value =  1000\n",
    "filtered_domain_counts = domain_name_counts[domain_name_counts > cutoff_value]\n",
    "print(filtered_domain_counts)\n",
    "\n",
    "filtered_username_counts = username_counts[username_counts > cutoff_value]\n",
    "print(filtered_username_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50a28cc7-0016-4159-82d7-c73caceea2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d25a5fd-b207-4197-b3ce-53fb1c33c38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.73.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "deb212af-3a6e-4a3d-a766-02697e15c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[DEBUG] Shape of numerical_features: (5116181, 6)\n",
      "[DEBUG] Identified top 100 users for one-hot encoding.\n",
      "[DEBUG] User feature columns: ['user_Anon84', 'user_BerislavLopac', 'user_Brajeshwar', 'user_CapitalistCartr', 'user_ColinWright', 'user_CrankyBear', 'user_DanielRibeiro', 'user_DiabloD3', 'user_DyslexicAtheist', 'user_Garbage', 'user_JumpCrisscross', 'user_LinuxBender', 'user_OTHER', 'user_PaulHoule', 'user_Tomte', 'user_aaronbrethorst', 'user_adamnemecek', 'user_adrian_mrd', 'user_amichail', 'user_anigbrowl', 'user_based2', 'user_belter', 'user_bookofjoe', 'user_bootload', 'user_bryanrasmussen', 'user_colinprince', 'user_coloneltcb', 'user_cwan', 'user_danso', 'user_dnetesn', 'user_doener', 'user_dsr12', 'user_edw519', 'user_edward', 'user_elorant', 'user_elsewhen', 'user_evo_9', 'user_fanf2', 'user_feross', 'user_fortran77', 'user_geox', 'user_ghosh', 'user_giuliomagnifico', 'user_gk1', 'user_gmays', 'user_happy-go-lucky', 'user_headalgorithm', 'user_hhs', 'user_howard941', 'user_iProject', 'user_iafrikan', 'user_ilamont', 'user_ingve', 'user_jgrahamc', 'user_jkuria', 'user_joeyespo', 'user_jonbaer', 'user_jseliger', 'user_kiyanwang', 'user_laurex', 'user_lelf', 'user_luu', 'user_marban', 'user_mfiguiere', 'user_mhb', 'user_mikece', 'user_mooreds', 'user_mpweiher', 'user_nickb', 'user_nreece', 'user_ohjeez', 'user_okket', 'user_pabs3', 'user_paulpauper', 'user_petethomas', 'user_prostoalex', 'user_protomyth', 'user_pseudolus', 'user_rbanffy', 'user_rmason', 'user_rntn', 'user_robg', 'user_samizdis', 'user_shawndumas', 'user_signa11', 'user_simonebrunozzi', 'user_smacktoward', 'user_sohkamyung', 'user_teleforce', 'user_thunderbong', 'user_todsacerdoti', 'user_tokenadult', 'user_tosh', 'user_uptown', 'user_vinnyglennon', 'user_wallflower', 'user_walterbell', 'user_wglb', 'user_wslh', 'user_yarapavan', 'user_zdw']\n",
      "[DEBUG] Shape of user_one_hot_features: (5116181, 101)\n",
      "[DEBUG] Identified top 100 domains for one-hot encoding.\n",
      "[DEBUG] Domain feature columns: ['domain_OTHER', 'domain_aeon.co', 'domain_apnews.com', 'domain_arstechnica.com', 'domain_arxiv.org', 'domain_aws.amazon.com', 'domain_bit.ly', 'domain_chrome.google.com', 'domain_dev.to', 'domain_docs.google.com', 'domain_edition.cnn.com', 'domain_en.wikipedia.org', 'domain_finance.yahoo.com', 'domain_fortune.com', 'domain_gigaom.com', 'domain_gist.github.com', 'domain_github.com', 'domain_gizmodo.com', 'domain_goo.gl', 'domain_hackaday.com', 'domain_hackernoon.com', 'domain_itunes.apple.com', 'domain_lwn.net', 'domain_mashable.com', 'domain_medium.com', 'domain_motherboard.vice.com', 'domain_nautil.us', 'domain_news.cnet.com', 'domain_news.ycombinator.com', 'domain_old.reddit.com', 'domain_online.wsj.com', 'domain_phys.org', 'domain_play.google.com', 'domain_qz.com', 'domain_spectrum.ieee.org', 'domain_stackoverflow.com', 'domain_techcrunch.com', 'domain_theconversation.com', 'domain_thenextweb.com', 'domain_torrentfreak.com', 'domain_twitter.com', 'domain_venturebeat.com', 'domain_vimeo.com', 'domain_www.axios.com', 'domain_www.bbc.co.uk', 'domain_www.bbc.com', 'domain_www.bloomberg.com', 'domain_www.businessinsider.com', 'domain_www.cbc.ca', 'domain_www.cnbc.com', 'domain_www.cnet.com', 'domain_www.cnn.com', 'domain_www.economist.com', 'domain_www.eff.org', 'domain_www.engadget.com', 'domain_www.facebook.com', 'domain_www.fastcompany.com', 'domain_www.forbes.com', 'domain_www.ft.com', 'domain_www.google.com', 'domain_www.guardian.co.uk', 'domain_www.huffingtonpost.com', 'domain_www.iafrikan.com', 'domain_www.independent.co.uk', 'domain_www.infoq.com', 'domain_www.kickstarter.com', 'domain_www.latimes.com', 'domain_www.linkedin.com', 'domain_www.macrumors.com', 'domain_www.nature.com', 'domain_www.newscientist.com', 'domain_www.newyorker.com', 'domain_www.npr.org', 'domain_www.nytimes.com', 'domain_www.phoronix.com', 'domain_www.quantamagazine.org', 'domain_www.quora.com', 'domain_www.readwriteweb.com', 'domain_www.reddit.com', 'domain_www.reuters.com', 'domain_www.sciencedaily.com', 'domain_www.scientificamerican.com', 'domain_www.sfgate.com', 'domain_www.slate.com', 'domain_www.slideshare.net', 'domain_www.techcrunch.com', 'domain_www.technologyreview.com', 'domain_www.telegraph.co.uk', 'domain_www.theatlantic.com', 'domain_www.theguardian.com', 'domain_www.theregister.co.uk', 'domain_www.theregister.com', 'domain_www.theverge.com', 'domain_www.vice.com', 'domain_www.vox.com', 'domain_www.washingtonpost.com', 'domain_www.wired.com', 'domain_www.wsj.com', 'domain_www.youtube.com', 'domain_www.zdnet.com', 'domain_youtu.be']\n",
      "[DEBUG] Shape of domain_one_hot_features: (5116181, 101)\n",
      "[DEBUG] Shape of target variable (y): (5116181, 1)\n",
      "\n",
      "PyTorch Neural Network Model Defined:\n",
      "HackerNewsPredictor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=208, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Loss function (MSE) and Optimizer (Adam) defined.\n",
      "\n",
      "Starting model training...\n",
      "Epoch [1/100], Train Loss: 3372.1386, Val Loss: 3244.5558\n",
      "Epoch [2/100], Train Loss: 3367.3425, Val Loss: 3242.4416\n",
      "Epoch [3/100], Train Loss: 3366.5216, Val Loss: 3244.3340\n",
      "Epoch [4/100], Train Loss: 3366.1765, Val Loss: 3243.0664\n",
      "Epoch [5/100], Train Loss: 3366.2701, Val Loss: 3242.6665\n",
      "Epoch [6/100], Train Loss: 3367.0675, Val Loss: 3245.7965\n",
      "Epoch [7/100], Train Loss: 3367.4765, Val Loss: 3243.6333\n",
      "Epoch [8/100], Train Loss: 3367.4861, Val Loss: 3240.8842\n",
      "Epoch [9/100], Train Loss: 3367.4007, Val Loss: 3243.5733\n",
      "Epoch [10/100], Train Loss: 3367.5737, Val Loss: 3242.3426\n",
      "Epoch [11/100], Train Loss: 3367.8769, Val Loss: 3241.0827\n",
      "Epoch [12/100], Train Loss: 3368.6200, Val Loss: 3244.6692\n",
      "Epoch [13/100], Train Loss: 3367.8975, Val Loss: 3242.0851\n",
      "Epoch [14/100], Train Loss: 3368.3364, Val Loss: 3241.8554\n",
      "Epoch [15/100], Train Loss: 3368.3320, Val Loss: 3240.8275\n",
      "Epoch [16/100], Train Loss: 3368.7293, Val Loss: 3242.6352\n",
      "Epoch [17/100], Train Loss: 3369.3456, Val Loss: 3243.1959\n",
      "Epoch [18/100], Train Loss: 3369.3448, Val Loss: 3242.6502\n",
      "Epoch [19/100], Train Loss: 3369.4217, Val Loss: 3247.0975\n",
      "Epoch [20/100], Train Loss: 3368.8884, Val Loss: 3244.4594\n",
      "Epoch [21/100], Train Loss: 3369.7371, Val Loss: 3244.1634\n",
      "Epoch [22/100], Train Loss: 3369.6537, Val Loss: 3243.1106\n",
      "Epoch [23/100], Train Loss: 3369.4066, Val Loss: 3242.8379\n",
      "Epoch [24/100], Train Loss: 3369.1110, Val Loss: 3247.3789\n",
      "Epoch [25/100], Train Loss: 3369.6483, Val Loss: 3243.0588\n",
      "Early stopping at epoch 25 as validation loss did not improve for 10 epochs.\n",
      "Model training finished.\n",
      "\n",
      "Model Evaluation on Test Set:\n",
      "Mean Squared Error (MSE): 3240.83\n",
      "Mean Absolute Error (MAE): 19.91\n",
      "R-squared (R²): 0.01\n",
      "\n",
      "Prediction for sample 0:\n",
      "Actual Upvotes: 1.0\n",
      "Predicted Upvotes: 13.29\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch # This is where torch is imported\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "df = pd.read_csv('RelevantDataScrape.csv', low_memory = False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "features = df[[\n",
    "    'score',\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars'\n",
    "]].values\n",
    "\n",
    "numerical_features = df[[\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars']].values\n",
    "print(f\"[DEBUG] Shape of numerical_features: {numerical_features.shape}\")\n",
    "\n",
    "if 'by' in df.columns:\n",
    "    num_top_users = 100\n",
    "    top_users = df['by'].value_counts().nlargest(num_top_users).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_users)} users for one-hot encoding.\")\n",
    "\n",
    "    # Create 'user_group' column with top users or 'OTHER'\n",
    "    df['user_group'] = df['by'].apply(lambda x: x if x in top_users else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    user_one_hot_features = pd.get_dummies(df['user_group'], prefix='user')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] User feature columns: {user_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of user_one_hot_features: {user_one_hot_features.shape}\")\n",
    "\n",
    "    user_one_hot_features_array = user_one_hot_features.values\n",
    "else:\n",
    "    user_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'by' column not available, skipping user one-hot encoding.\")\n",
    "\n",
    "# --- DOMAINS ---\n",
    "\n",
    "if 'domain_name' in df.columns:\n",
    "    num_top_domain = 100\n",
    "    top_domain = df['domain_name'].value_counts().nlargest(num_top_domain).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_domain)} domains for one-hot encoding.\")\n",
    "\n",
    "    # Create 'domain_group' column with top domains or 'OTHER'\n",
    "    df['domain_group'] = df['domain_name'].apply(lambda x: x if x in top_domain else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    domain_one_hot_features = pd.get_dummies(df['domain_group'], prefix='domain')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] Domain feature columns: {domain_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of domain_one_hot_features: {domain_one_hot_features.shape}\")\n",
    "\n",
    "    domain_one_hot_features_array = domain_one_hot_features.values\n",
    "else:\n",
    "    domain_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'domain_name' column not available, skipping domain one-hot encoding.\")\n",
    "\n",
    "# Combine features and target\n",
    "X = np.hstack((numerical_features, user_one_hot_features_array, domain_one_hot_features_array))\n",
    "y = df[['score']].values.reshape(-1, 1)\n",
    "print(f\"[DEBUG] Shape of target variable (y): {y.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Shuffle and manually split 80/20\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "split = int(num_samples * 0.8)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "X_train_tensor = X_tensor[train_indices]\n",
    "y_train_tensor = y_tensor[train_indices]\n",
    "X_test_tensor = X_tensor[test_indices]\n",
    "y_test_tensor = y_tensor[test_indices]\n",
    "\n",
    "# Normalize using training set stats\n",
    "mean = X_train_tensor.mean(dim=0, keepdim=True)\n",
    "std = X_train_tensor.std(dim=0, keepdim=True)\n",
    "std[std == 0] = 1.0  # Prevent division by zero\n",
    "\n",
    "X_train_tensor = (X_train_tensor - mean) / std\n",
    "X_test_tensor = (X_test_tensor - mean) / std  # Use train set stats\n",
    "\n",
    "# Move to device (CPU or GPU)\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- 5. Define the Neural Network Model in PyTorch ---\n",
    "# Refactored to use nn.Sequential\n",
    "class HackerNewsPredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HackerNewsPredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(), # Activation function for the first hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), # Activation function for the second hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1) # Output layer with linear activation (default for nn.Linear)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = HackerNewsPredictor(input_dim).to(device) # Move model to device (CPU/GPU)\n",
    "\n",
    "print(\"\\nPyTorch Neural Network Model Defined:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# --- 6. Define Loss Function and Optimizer ---\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam optimizer with learning rate\n",
    "\n",
    "print(\"\\nLoss function (MSE) and Optimizer (Adam) defined.\")\n",
    "\n",
    "\n",
    "# --- 7. Train the Model ---\n",
    "num_epochs = 100 # Max epochs, EarlyStopping will manage it\n",
    "patience = 10 # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # Move data to device\n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, targets) # Calculate loss\n",
    "        loss.backward() # Backward pass (compute gradients)\n",
    "        optimizer.step() # Update weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_actuals = []\n",
    "    with torch.no_grad(): # Disable gradient calculations during validation\n",
    "        for inputs, targets in test_loader: # Using test_loader for simplicity in this example for validation\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            val_running_loss += val_loss.item() * inputs.size(0)\n",
    "            val_predictions.extend(outputs.cpu().numpy())\n",
    "            val_actuals.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_dataset) # Use test_dataset for size calculation here\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping check\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth') # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "print(\"Model training finished.\")\n",
    "\n",
    "\n",
    "# --- 8. Evaluate the Model on the Test Set (or load best model and evaluate) ---\n",
    "model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_actuals.append(targets)\n",
    "\n",
    "# Concatenate tensors\n",
    "all_predictions = torch.cat(all_predictions).flatten()\n",
    "all_actuals = torch.cat(all_actuals).flatten()\n",
    "\n",
    "# Compute metrics using PyTorch only\n",
    "mse = torch.mean((all_predictions - all_actuals) ** 2).item()\n",
    "mae = torch.mean(torch.abs(all_predictions - all_actuals)).item()\n",
    "\n",
    "# R² calculation: 1 - SSR/SST\n",
    "ss_res = torch.sum((all_predictions - all_actuals) ** 2)\n",
    "ss_tot = torch.sum((all_actuals - torch.mean(all_actuals)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot).item()\n",
    "\n",
    "print(f\"\\nModel Evaluation on Test Set:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "# --- 9. Make Predictions (Optional) ---\n",
    "# Prepare a sample for prediction (e.g., the first sample from the test set)\n",
    "sample_index = 0\n",
    "sample_input_scaled = X_test_tensor[sample_index:sample_index+1]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_upvotes_tensor = model(sample_input_scaled)\n",
    "    predicted_upvotes = predicted_upvotes_tensor.item()  # Convert to scalar\n",
    "\n",
    "# Actual value (assuming y_test is a PyTorch tensor or converted here)\n",
    "actual_upvotes = y_test_tensor[sample_index].item()\n",
    "\n",
    "print(f\"\\nPrediction for sample {sample_index}:\")\n",
    "print(f\"Actual Upvotes: {actual_upvotes}\")\n",
    "print(f\"Predicted Upvotes: {predicted_upvotes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a59f6-d5e1-4815-ad20-00378a9781cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd51c3f-530b-4a65-8e2a-616ea607500c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
