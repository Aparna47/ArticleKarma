{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f2c2764-577e-4a4f-b298-0143170cfffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Users/clementha/anaconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7385ecb-6a10-45a7-8fcf-37463ddeb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_csv(\"Inputs1stGen.csv\", low_memory = False)\n",
    "df = pd.read_csv(\"Inputs1stGen.csv\", low_memory = False, nrows = 100000)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c63e32c-2edd-4fa3-9560-25ee4635c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mclemha\u001b[0m (\u001b[33mclemha-mli\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/clementha/Documents/MLX1/ArticleKarma/wandb/run-20250613_110625-5bpgjbqk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/clemha-mli/HackerNews%20Prediction%20from%20HyperParameterHippies/runs/5bpgjbqk' target=\"_blank\">glad-wave-2</a></strong> to <a href='https://wandb.ai/clemha-mli/HackerNews%20Prediction%20from%20HyperParameterHippies' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/clemha-mli/HackerNews%20Prediction%20from%20HyperParameterHippies' target=\"_blank\">https://wandb.ai/clemha-mli/HackerNews%20Prediction%20from%20HyperParameterHippies</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/clemha-mli/HackerNews%20Prediction%20from%20HyperParameterHippies/runs/5bpgjbqk' target=\"_blank\">https://wandb.ai/clemha-mli/HackerNews%20Prediction%20from%20HyperParameterHippies/runs/5bpgjbqk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# W&B Integration\n",
    "import wandb\n",
    "run = wandb.init(\n",
    "    # Set the wandb entity where your project will be logged (generally your team name).\n",
    "    entity=\"clemha-mli\", # replace this to your own W&B account\n",
    "    # Set the wandb project where this run will be logged.\n",
    "    project=\"HackerNews Prediction from HyperParameterHippies\",\n",
    "    # Track hyperparameters and run metadata.\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"architecture\": \"Our architecture\",\n",
    "        \"dataset\": \"Hackernews text and a lot\",\n",
    "        \"epochs\": 30,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c577c4c2-baa0-4743-ae47-d5404b4be06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "'''\n",
    "The following section gives us all the parameters relating to time:\n",
    "1. Day of week\n",
    "2. Time of day\n",
    "3. Month of Year\n",
    "4. Year of past \n",
    "'''\n",
    "\n",
    "#Converting UNIX timestamp to datetime\n",
    "df['datetime'] = pd.to_datetime(df['time'])\n",
    "\n",
    "#Getting the names of the days of the week\n",
    "df['day_of_week'] = df['datetime'].dt.day_name()\n",
    "\n",
    "#Creating a key of days of the week to numbers\n",
    "day_map = {\n",
    "    'Monday': 0, 'Tuesday': 1, 'Wednesday':2, 'Thursday':3, 'Friday':4, 'Saturday':5, 'Sunday':6\n",
    "}\n",
    "\n",
    "#Assigning numbers to the days of the week corresponding to the dataset\n",
    "df[\"day_of_week_num\"] = df['day_of_week'].map(day_map)\n",
    "\n",
    "#Getting the names of the months of year\n",
    "df['month'] = df['datetime'].dt.month_name()\n",
    "\n",
    "#Creating a key of months of the year to numbers\n",
    "month_map = {\n",
    "    'January': 1, 'February': 2, 'March': 3, 'April':4, 'May': 5, 'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October':10, 'November': 11, 'December': 12\n",
    "}\n",
    "\n",
    "df['month_num'] = df['month'].map(month_map)\n",
    "\n",
    "#Getting the hour of the post\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "#Getting the year of the post\n",
    "df['year'] = df['datetime'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26532795-6b37-4dd8-8bdd-a6e78cb43e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   by   karma  \\\n",
      "0         ColinWright  127765   \n",
      "1                hncj      44   \n",
      "2          andymboyle     714   \n",
      "3             digisth    2395   \n",
      "4             jazzdev     543   \n",
      "...               ...     ...   \n",
      "99995  ahmetalpbalkan      97   \n",
      "99996          cocoon      49   \n",
      "99997   thestartupguy       5   \n",
      "99998         ph0rque   23296   \n",
      "99999        cshekhar     117   \n",
      "\n",
      "                                                   title  \\\n",
      "0      \"What May Happen in the Next Hundred Years\", f...   \n",
      "1           Getting Started with JavaScript Unit Testing   \n",
      "2      Armstrong, the Django-based and open-source ne...   \n",
      "3                   Why Web Reviewers Make Up Bad Things   \n",
      "4      You Weren't Meant to Have a Boss: The Cliff Notes   \n",
      "...                                                  ...   \n",
      "99995  Bookmarklet: Hide MG Siegler's posts on TechCr...   \n",
      "99996           Who owns the right to analyze your data?   \n",
      "99997  Freelancer or Entrepreneur? Seth Godin cares a...   \n",
      "99998  A letter from Felisberto Tole (RE: The $300 Ho...   \n",
      "99999     Infamous PS3, iPhone hacker hired by Facebook    \n",
      "\n",
      "                                                     url                 time  \\\n",
      "0      http://www.howtobearetronaut.com/wp-content/up...  2011-10-24 16:27:00   \n",
      "1      http://blogs.lessthandot.com/index.php/WebDev/...  2012-01-23 11:39:25   \n",
      "2      http://www.marketwatch.com/story/the-bay-citiz...  2011-10-24 16:27:36   \n",
      "3      http://bits.blogs.nytimes.com/2013/07/15/why-w...  2013-07-16 05:16:26   \n",
      "4                   http://paulgraham.com/bossnotes.html  2008-03-30 09:46:25   \n",
      "...                                                  ...                  ...   \n",
      "99995                               http://rpa.tl/hidemg  2011-07-25 22:08:37   \n",
      "99996  http://blog.getcocoon.com/2011/07/25/who-owns-...  2011-07-25 22:09:39   \n",
      "99997  http://beingremarkable.me/blog/freelancer-or-e...  2013-06-19 10:53:01   \n",
      "99998  http://www.300house.com/blog/2011/06/awaken-mo...  2011-06-28 09:38:10   \n",
      "99999  http://www.businessinsider.com/infamous-ps3-ip...  2011-06-28 09:40:57   \n",
      "\n",
      "       score            datetime day_of_week  day_of_week_num    month  \\\n",
      "0         19 2011-10-24 16:27:00      Monday                0  October   \n",
      "1          1 2012-01-23 11:39:25      Monday                0  January   \n",
      "2          2 2011-10-24 16:27:36      Monday                0  October   \n",
      "3          1 2013-07-16 05:16:26     Tuesday                1     July   \n",
      "4          1 2008-03-30 09:46:25      Sunday                6    March   \n",
      "...      ...                 ...         ...              ...      ...   \n",
      "99995     12 2011-07-25 22:08:37      Monday                0     July   \n",
      "99996      1 2011-07-25 22:09:39      Monday                0     July   \n",
      "99997      1 2013-06-19 10:53:01   Wednesday                2     June   \n",
      "99998      1 2011-06-28 09:38:10     Tuesday                1     June   \n",
      "99999      1 2011-06-28 09:40:57     Tuesday                1     June   \n",
      "\n",
      "       month_num  hour  year  \n",
      "0             10    16  2011  \n",
      "1              1    11  2012  \n",
      "2             10    16  2011  \n",
      "3              7     5  2013  \n",
      "4              3     9  2008  \n",
      "...          ...   ...   ...  \n",
      "99995          7    22  2011  \n",
      "99996          7    22  2011  \n",
      "99997          6    10  2013  \n",
      "99998          6     9  2011  \n",
      "99999          6     9  2011  \n",
      "\n",
      "[93708 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eac54ae-e678-4ba5-b8f2-c4b7c95a6f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 16:59:00\n"
     ]
    }
   ],
   "source": [
    "max_time = df['datetime'].max()\n",
    "print(max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a6eb87-f621-4557-ae6c-1ba2265a1e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_since_post1'] = max_time - df['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38c3c575-5441-45a6-9aea-4597e7699695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       4511 days 00:32:00\n",
      "1       4420 days 05:19:35\n",
      "2       4511 days 00:31:24\n",
      "3       3880 days 11:42:34\n",
      "4       5814 days 07:12:35\n",
      "               ...        \n",
      "99995   4601 days 18:50:23\n",
      "99996   4601 days 18:49:21\n",
      "99997   3907 days 06:05:59\n",
      "99998   4629 days 07:20:50\n",
      "99999   4629 days 07:18:03\n",
      "Name: time_since_post1, Length: 93708, dtype: timedelta64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7a59f6-d5e1-4815-ad20-00378a9781cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Switched to eval mode. Pre-warming cache... ---\n",
      "--- Calculating and caching normalized embeddings... ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Word2Vec(\n",
       "  (in_embeddings): Embedding(71290, 50)\n",
       "  (out_embeddings): Linear(in_features=50, out_features=71290, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib8 import Word2Vec\n",
    "import torch\n",
    "\n",
    "embedding_dim = 50\n",
    "device_ids = [0, 1] # Use the first two GPUs\n",
    "device = torch.device(f\"cuda:{device_ids[0]}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Initialize Model, Loss, and Optimizer ---\n",
    "MODEL_PATH = f\"w2v/word2vec_pytorch_{embedding_dim}.pth\"\n",
    "\n",
    "model = Word2Vec.load_from_checkpoint(MODEL_PATH, device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3560e65a-3c01-4719-8df3-cdbf8155c4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using cached embeddings. ---\n",
      "tensor([ 0.0151, -0.2932,  0.0731, -0.0389, -0.0133,  0.0778,  0.2062,  0.0685,\n",
      "         0.1959,  0.2224,  0.2976,  0.0808,  0.0589, -0.2884,  0.0526, -0.1409,\n",
      "        -0.0912, -0.1140, -0.1360, -0.2357,  0.3193, -0.0581,  0.0759, -0.0365,\n",
      "        -0.1061, -0.0400,  0.1124, -0.1369, -0.0608, -0.0191, -0.1755,  0.2196,\n",
      "         0.1463, -0.1008, -0.0974,  0.0120, -0.1018,  0.0551, -0.0144,  0.1746,\n",
      "         0.0509, -0.1130, -0.2061, -0.0268,  0.1000, -0.1205, -0.1274,  0.0267,\n",
      "         0.2003, -0.0287])\n",
      "--- Using cached embeddings. ---\n",
      "------------------------------\n",
      "is                   0.7491\n",
      "alive                0.6933\n",
      "was                  0.5683\n",
      "punchline            0.5467\n",
      "happens              0.5366\n",
      "isn                  0.5364\n",
      "stands               0.5295\n",
      "lop                  0.5261\n",
      "makes                0.5194\n",
      "distasteful          0.5169\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "similar_words = model.find_most_similar('germany', top_n=10)\n",
    "# similar_words = model.find_most_similar('computer', top_n=10)\n",
    "\n",
    "# p = model.get_vector('paris')\n",
    "# f = model.get_vector('france')\n",
    "# g = model.get_vector('germany')\n",
    "\n",
    "f = model.get_sentence_vector('Elon Musk is alive')\n",
    "\n",
    "# bgf = p - f + g\n",
    "print(f)\n",
    "similar_words = model.find_most_similar_by_vector(f, top_n=10)\n",
    "\n",
    "if similar_words:\n",
    "    print(\"-\" * 30)\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word:<20} {score:.4f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0a2df4b-05ce-47b6-b453-8c456cc4ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "2. Time since the post was posted\n",
    "'''\n",
    "\n",
    "df['time_since_post'] = df['time_since_post1'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d564d13e-0814-47d8-8cc3-54986dfb71b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        389752320.0\n",
      "1        381907175.0\n",
      "2        389752284.0\n",
      "3        335274154.0\n",
      "4        502355555.0\n",
      "            ...     \n",
      "99995    397594223.0\n",
      "99996    397594161.0\n",
      "99997    337586759.0\n",
      "99998    399972050.0\n",
      "99999    399971883.0\n",
      "Name: time_since_post, Length: 93708, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df['time_since_post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d433c7b-6ed7-40e3-a16e-ea54892c6348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing URLs: 100%|██████████████████| 93708/93708 [00:02<00:00, 41006.01it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. URL\n",
    "2. Domain\n",
    "'''\n",
    "\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas(desc=\"Parsing URLs\")\n",
    "\n",
    "# Define a safe parser\n",
    "def safe_urlparse(url):\n",
    "    try: \n",
    "        parsed = urlparse(url)\n",
    "        return parsed.netloc, parsed.path\n",
    "    except Exception:\n",
    "        return '', ''\n",
    "\n",
    "# Ensure URL column is clean\n",
    "df['url'] = df['url'].fillna('').astype(str)\n",
    "\n",
    "# Apply with progress bar\n",
    "df[['domain_name', 'url_path']] = df['url'].progress_apply(\n",
    "    lambda u: pd.Series(safe_urlparse(u))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813127ec-43e3-4fc5-8d2c-29460b123364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chatgpt.com', '/c/684813b3-5bfc-8013-8f49e')\n"
     ]
    }
   ],
   "source": [
    "print(safe_urlparse('https://chatgpt.com/c/684813b3-5bfc-8013-8f49e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83995136-b25a-4ae1-b361-22772f061673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        http://www.howtobearetronaut.com/wp-content/up...\n",
      "1        http://blogs.lessthandot.com/index.php/WebDev/...\n",
      "2        http://www.marketwatch.com/story/the-bay-citiz...\n",
      "3        http://bits.blogs.nytimes.com/2013/07/15/why-w...\n",
      "4                     http://paulgraham.com/bossnotes.html\n",
      "                               ...                        \n",
      "99995                                 http://rpa.tl/hidemg\n",
      "99996    http://blog.getcocoon.com/2011/07/25/who-owns-...\n",
      "99997    http://beingremarkable.me/blog/freelancer-or-e...\n",
      "99998    http://www.300house.com/blog/2011/06/awaken-mo...\n",
      "99999    http://www.businessinsider.com/infamous-ps3-ip...\n",
      "Name: url, Length: 93708, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b474c7dc-7120-4bd0-a210-3637e14ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code gives us:\n",
    "1. User name\n",
    "2. Title\n",
    "3. Length of Title\n",
    "4. Number of Upvotes\n",
    "'''\n",
    "\n",
    "df['by'] = df['by'].fillna('').astype(str)\n",
    "df['title'] = df['title'].fillna('').astype(str)\n",
    "df['title_length_chars'] = df['title'].str.len()\n",
    "df['title_length_words'] = df['title'].str.split().str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e63fec9-53e1-43f8-971d-e7e7e48275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting a file that has:\n",
    "- User ('by')\n",
    "- Title ('title')\n",
    "- Domain ('domain_name')\n",
    "- Day of the week ('day_of_week_num')\n",
    "- Month ('month')\n",
    "- Hour ('hour')\n",
    "- Year ('year')\n",
    "- Time since post ('time_since_post')\n",
    "- Title length chars ('title_length_chars')\n",
    "- Score ('score')\n",
    "'''\n",
    "\n",
    "selected_columns = ['by', 'title', 'domain_name', 'day_of_week_num', 'month_num', 'hour', 'year', 'time_since_post', 'title_length_chars', 'score']\n",
    "df_selected = df[selected_columns]\n",
    "df_selected.to_csv('RelevantDataScrape.csv', index=False)\n",
    "\n",
    "df['domain_name'].value_counts()\n",
    "domain_name_counts = df['domain_name'].value_counts()\n",
    "df['by'].value_counts()\n",
    "username_counts = df['by'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65c7fff1-ae98-440c-b6ee-d1fb55e61d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count\n",
      "count\n"
     ]
    }
   ],
   "source": [
    "domaincolumn_names = domain_name_counts.name\n",
    "print(domaincolumn_names)\n",
    "usernamecolumn_names = username_counts.name\n",
    "print(usernamecolumn_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b25468a-d842-425b-af45-2d7c3236186c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain_name\n",
      "techcrunch.com     1282\n",
      "www.youtube.com    1220\n",
      "www.nytimes.com    1066\n",
      "github.com         1013\n",
      "Name: count, dtype: int64\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "cutoff_value =  1000\n",
    "filtered_domain_counts = domain_name_counts[domain_name_counts > cutoff_value]\n",
    "print(filtered_domain_counts)\n",
    "\n",
    "filtered_username_counts = username_counts[username_counts > cutoff_value]\n",
    "print(filtered_username_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f7b224f-4b31-45dd-ab10-2213f8ad604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # This is where torch is imported\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "084532f7-67b1-4a5f-b007-04505f4537a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RelevantDataScrape.csv', low_memory = False)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ec72bb9-866c-4948-80dd-ece93c462aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating sentence embeddings: 100%|█| 93707/93707 [00:02<00:00, 37361.58it/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] title_embeddings.shape = (93707, 50)\n"
     ]
    }
   ],
   "source": [
    "sentence_vectors = []\n",
    "for title in tqdm(df['title'], desc=\"Generating sentence embeddings\"):\n",
    "    with torch.no_grad():\n",
    "        vector = model.get_sentence_vector(str(title)).cpu().numpy()  # ensure numpy\n",
    "        sentence_vectors.append(vector)\n",
    "\n",
    "# Stack into a matrix (shape: num_samples x embedding_dim)\n",
    "title_embeddings = np.vstack(sentence_vectors)\n",
    "print(f\"[DEBUG] title_embeddings.shape = {title_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "deb212af-3a6e-4a3d-a766-02697e15c1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[DEBUG] Shape of numerical_features: (93707, 6)\n",
      "[DEBUG] Identified top 100 users for one-hot encoding.\n",
      "[DEBUG] User feature columns: ['user_Anon84', 'user_ColinWright', 'user_DanielRibeiro', 'user_FluidDjango', 'user_Garbage', 'user_J3L2404', 'user_JumpCrisscross', 'user_Libertatea', 'user_MikeCapone', 'user_OTHER', 'user_PatriciaLopes', 'user_RiderOfGiraffes', 'user_Tsiolkovsky', 'user_aaronbrethorst', 'user_abennett', 'user_abraham', 'user_alexandros', 'user_amichail', 'user_anigbrowl', 'user_antr', 'user_apievangelist', 'user_bdfh42', 'user_bensummers', 'user_bjonathan', 'user_bootload', 'user_ca98am79', 'user_canistr', 'user_chaostheory', 'user_cleverjake', 'user_coderdude', 'user_cwan', 'user_danielha', 'user_danso', 'user_danw', 'user_darkduck', 'user_davidw', 'user_denzil_correa', 'user_digiwizard', 'user_duck', 'user_dwynings', 'user_edw519', 'user_evo_9', 'user_fogus', 'user_fromedome', 'user_georgecmu', 'user_gibsonf1', 'user_gnosis', 'user_grellas', 'user_helwr', 'user_hhm', 'user_iProject', 'user_iamelgringo', 'user_ilamont', 'user_jacquesm', 'user_jamesbritt', 'user_jaybol', 'user_jmorin007', 'user_joeyespo', 'user_joschi', 'user_joshuacc', 'user_llambda', 'user_lmacvittie', 'user_markbao', 'user_martyzwilling', 'user_messel', 'user_mhb', 'user_mikecane', 'user_mtgx', 'user_naish', 'user_nextparadigms', 'user_nickb', 'user_nickstamoulis', 'user_nreece', 'user_peter123', 'user_pg', 'user_pgalih', 'user_ph0rque', 'user_pius', 'user_prakash', 'user_pwg', 'user_python_kiss', 'user_revorad', 'user_rms', 'user_robg', 'user_shawndumas', 'user_ssclafani', 'user_taylorbuley', 'user_terpua', 'user_thankuz', 'user_tokenadult', 'user_transburgh', 'user_twampss', 'user_ukdm', 'user_unixroot', 'user_vaksel', 'user_wallflower', 'user_wglb', 'user_wireheadlance', 'user_wslh', 'user_yarapavan', 'user_zoowar']\n",
      "[DEBUG] Shape of user_one_hot_features: (93707, 101)\n",
      "[DEBUG] Identified top 100 domains for one-hot encoding.\n",
      "[DEBUG] Domain feature columns: ['domain_37signals.com', 'domain_OTHER', 'domain_allthingsd.com', 'domain_arstechnica.com', 'domain_bit.ly', 'domain_bits.blogs.nytimes.com', 'domain_blog.startupprofessionals.com', 'domain_blogs.hbr.org', 'domain_blogs.msdn.com', 'domain_blogs.wsj.com', 'domain_code.google.com', 'domain_devcentral.f5.com', 'domain_en.wikipedia.org', 'domain_gigaom.com', 'domain_gist.github.com', 'domain_github.com', 'domain_gizmodo.com', 'domain_googleblog.blogspot.com', 'domain_groups.google.com', 'domain_itunes.apple.com', 'domain_lifehacker.com', 'domain_mashable.com', 'domain_medium.com', 'domain_money.cnn.com', 'domain_news.bbc.co.uk', 'domain_news.cnet.com', 'domain_news.yahoo.com', 'domain_online.wsj.com', 'domain_pandodaily.com', 'domain_plus.google.com', 'domain_radar.oreilly.com', 'domain_sethgodin.typepad.com', 'domain_spectrum.ieee.org', 'domain_stackoverflow.com', 'domain_techcrunch.com', 'domain_thehackernews.com', 'domain_thenextweb.com', 'domain_torrentfreak.com', 'domain_twitter.com', 'domain_venturebeat.com', 'domain_vimeo.com', 'domain_www.alleyinsider.com', 'domain_www.appleinsider.com', 'domain_www.avc.com', 'domain_www.bbc.co.uk', 'domain_www.bgr.com', 'domain_www.bloomberg.com', 'domain_www.businessinsider.com', 'domain_www.businessweek.com', 'domain_www.cnn.com', 'domain_www.computerworld.com', 'domain_www.computerworlduk.com', 'domain_www.economist.com', 'domain_www.eff.org', 'domain_www.engadget.com', 'domain_www.extremetech.com', 'domain_www.facebook.com', 'domain_www.fastcompany.com', 'domain_www.forbes.com', 'domain_www.gamasutra.com', 'domain_www.geek.com', 'domain_www.google.com', 'domain_www.guardian.co.uk', 'domain_www.howtoforge.com', 'domain_www.huffingtonpost.com', 'domain_www.inc.com', 'domain_www.infoq.com', 'domain_www.infosecisland.com', 'domain_www.infoworld.com', 'domain_www.itworld.com', 'domain_www.kickstarter.com', 'domain_www.latimes.com', 'domain_www.macobserver.com', 'domain_www.networkworld.com', 'domain_www.newscientist.com', 'domain_www.newyorker.com', 'domain_www.npr.org', 'domain_www.nytimes.com', 'domain_www.pcmag.com', 'domain_www.pcworld.com', 'domain_www.quora.com', 'domain_www.readwriteweb.com', 'domain_www.reddit.com', 'domain_www.reuters.com', 'domain_www.sciencedaily.com', 'domain_www.securityweek.com', 'domain_www.sfgate.com', 'domain_www.slate.com', 'domain_www.slideshare.net', 'domain_www.techcrunch.com', 'domain_www.techdirt.com', 'domain_www.technologyreview.com', 'domain_www.telegraph.co.uk', 'domain_www.theatlantic.com', 'domain_www.theregister.co.uk', 'domain_www.theverge.com', 'domain_www.washingtonpost.com', 'domain_www.wired.com', 'domain_www.xconomy.com', 'domain_www.youtube.com', 'domain_www.zdnet.com']\n",
      "[DEBUG] Shape of domain_one_hot_features: (93707, 101)\n",
      "[DEBUG] Shape of target variable (y): (93707, 1)\n",
      "\n",
      "PyTorch Neural Network Model Defined:\n",
      "HackerNewsPredictor(\n",
      "  (network): Sequential(\n",
      "    (0): Linear(in_features=258, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "features = df[[\n",
    "    'score',\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars'\n",
    "]].values\n",
    "\n",
    "numerical_features = df[[\n",
    "    'day_of_week_num',\n",
    "    'month_num',\n",
    "    'hour',\n",
    "    'year',\n",
    "    'time_since_post',\n",
    "    'title_length_chars']].values\n",
    "print(f\"[DEBUG] Shape of numerical_features: {numerical_features.shape}\")\n",
    "\n",
    "if 'by' in df.columns:\n",
    "    num_top_users = 100\n",
    "    top_users = df['by'].value_counts().nlargest(num_top_users).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_users)} users for one-hot encoding.\")\n",
    "\n",
    "    # Create 'user_group' column with top users or 'OTHER'\n",
    "    df['user_group'] = df['by'].apply(lambda x: x if x in top_users else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    user_one_hot_features = pd.get_dummies(df['user_group'], prefix='user')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] User feature columns: {user_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of user_one_hot_features: {user_one_hot_features.shape}\")\n",
    "\n",
    "    user_one_hot_features_array = user_one_hot_features.values\n",
    "else:\n",
    "    user_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'by' column not available, skipping user one-hot encoding.\")\n",
    "\n",
    "# --- DOMAINS ---\n",
    "\n",
    "if 'domain_name' in df.columns:\n",
    "    num_top_domain = 100\n",
    "    top_domain = df['domain_name'].value_counts().nlargest(num_top_domain).index.tolist()\n",
    "    print(f\"[DEBUG] Identified top {len(top_domain)} domains for one-hot encoding.\")\n",
    "\n",
    "    # Create 'domain_group' column with top domains or 'OTHER'\n",
    "    df['domain_group'] = df['domain_name'].apply(lambda x: x if x in top_domain else 'OTHER')\n",
    "\n",
    "    # One-hot encode\n",
    "    domain_one_hot_features = pd.get_dummies(df['domain_group'], prefix='domain')\n",
    "\n",
    "    # Debug\n",
    "    print(f\"[DEBUG] Domain feature columns: {domain_one_hot_features.columns.tolist()}\")\n",
    "    print(f\"[DEBUG] Shape of domain_one_hot_features: {domain_one_hot_features.shape}\")\n",
    "\n",
    "    domain_one_hot_features_array = domain_one_hot_features.values\n",
    "else:\n",
    "    domain_one_hot_features_array = np.empty((len(df), 0))\n",
    "    print(\"[DEBUG] 'domain_name' column not available, skipping domain one-hot encoding.\")\n",
    "\n",
    "# Combine features and target\n",
    "X = np.hstack((numerical_features, user_one_hot_features_array, domain_one_hot_features_array, title_embeddings))\n",
    "y = df[['score']].values.reshape(-1, 1)\n",
    "print(f\"[DEBUG] Shape of target variable (y): {y.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Shuffle and manually split 80/20\n",
    "num_samples = X_tensor.shape[0]\n",
    "indices = torch.randperm(num_samples)\n",
    "split = int(num_samples * 0.8)\n",
    "\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "X_train_tensor = X_tensor[train_indices]\n",
    "y_train_tensor = y_tensor[train_indices]\n",
    "X_test_tensor = X_tensor[test_indices]\n",
    "y_test_tensor = y_tensor[test_indices]\n",
    "\n",
    "# Normalize using training set stats\n",
    "mean = X_train_tensor.mean(dim=0, keepdim=True)\n",
    "std = X_train_tensor.std(dim=0, keepdim=True)\n",
    "std[std == 0] = 1.0  # Prevent division by zero\n",
    "\n",
    "X_train_tensor = (X_train_tensor - mean) / std\n",
    "X_test_tensor = (X_test_tensor - mean) / std  # Use train set stats\n",
    "\n",
    "# Move to device (CPU or GPU)\n",
    "X_train_tensor = X_train_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- 5. Define the Neural Network Model in PyTorch ---\n",
    "# Refactored to use nn.Sequential\n",
    "class HackerNewsPredictor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(HackerNewsPredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(), # Activation function for the first hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(), # Activation function for the second hidden layer\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1) # Output layer with linear activation (default for nn.Linear)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = HackerNewsPredictor(input_dim).to(device) # Move model to device (CPU/GPU)\n",
    "\n",
    "print(\"\\nPyTorch Neural Network Model Defined:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34b2c9ef-570b-463f-8cb9-d3260a66a0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function (MSE) and Optimizer (Adam) defined.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Define Loss Function and Optimizer ---\n",
    "criterion = nn.MSELoss() # Mean Squared Error Loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config['learning_rate']) # Adam optimizer with learning rate\n",
    "\n",
    "print(\"\\nLoss function (MSE) and Optimizer (Adam) defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04307e44-1ea0-4285-8eeb-98179a5fdf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch [1/100], Train Loss: 1338.2567, Val Loss: 1615.6520\n",
      "Epoch [2/100], Train Loss: 1326.2476, Val Loss: 1616.1658\n",
      "Epoch [3/100], Train Loss: 1321.7515, Val Loss: 1616.6325\n",
      "Epoch [4/100], Train Loss: 1320.7929, Val Loss: 1618.7513\n",
      "Epoch [5/100], Train Loss: 1316.9074, Val Loss: 1616.6979\n",
      "Epoch [6/100], Train Loss: 1315.3552, Val Loss: 1614.0039\n",
      "Epoch [7/100], Train Loss: 1313.6928, Val Loss: 1615.9708\n",
      "Epoch [8/100], Train Loss: 1310.8122, Val Loss: 1617.7590\n",
      "Epoch [9/100], Train Loss: 1311.7611, Val Loss: 1616.1585\n",
      "Epoch [10/100], Train Loss: 1307.9131, Val Loss: 1617.6413\n",
      "Epoch [11/100], Train Loss: 1305.7236, Val Loss: 1623.5347\n",
      "Epoch [12/100], Train Loss: 1302.2046, Val Loss: 1616.8567\n",
      "Epoch [13/100], Train Loss: 1298.4147, Val Loss: 1617.9507\n",
      "Epoch [14/100], Train Loss: 1292.0043, Val Loss: 1621.3748\n",
      "Epoch [15/100], Train Loss: 1286.7415, Val Loss: 1621.6066\n",
      "Epoch [16/100], Train Loss: 1282.1002, Val Loss: 1625.7407\n",
      "Epoch [17/100], Train Loss: 1274.7641, Val Loss: 1624.4165\n",
      "Epoch [18/100], Train Loss: 1266.6949, Val Loss: 1627.3319\n",
      "Epoch [19/100], Train Loss: 1257.6349, Val Loss: 1630.1111\n",
      "Epoch [20/100], Train Loss: 1244.3046, Val Loss: 1632.0454\n",
      "Epoch [21/100], Train Loss: 1234.3909, Val Loss: 1629.2072\n",
      "Epoch [22/100], Train Loss: 1213.3452, Val Loss: 1632.8840\n",
      "Epoch [23/100], Train Loss: 1197.6698, Val Loss: 1636.1423\n",
      "Epoch [24/100], Train Loss: 1180.6920, Val Loss: 1635.6946\n",
      "Epoch [25/100], Train Loss: 1172.4620, Val Loss: 1637.4932\n",
      "Epoch [26/100], Train Loss: 1170.1463, Val Loss: 1628.5531\n",
      "Epoch [27/100], Train Loss: 1144.5238, Val Loss: 1636.9088\n",
      "Epoch [28/100], Train Loss: 1154.1603, Val Loss: 1637.6166\n",
      "Epoch [29/100], Train Loss: 1131.8574, Val Loss: 1631.6500\n",
      "Epoch [30/100], Train Loss: 1132.6436, Val Loss: 1638.6661\n",
      "Epoch [31/100], Train Loss: 1111.7148, Val Loss: 1639.1638\n",
      "Epoch [32/100], Train Loss: 1109.8625, Val Loss: 1629.3082\n",
      "Epoch [33/100], Train Loss: 1109.0134, Val Loss: 1630.3276\n",
      "Epoch [34/100], Train Loss: 1107.2006, Val Loss: 1638.6533\n",
      "Epoch [35/100], Train Loss: 1096.7789, Val Loss: 1628.4085\n",
      "Epoch [36/100], Train Loss: 1109.6935, Val Loss: 1635.8605\n",
      "Early stopping at epoch 36 as validation loss did not improve for 30 epochs.\n",
      "Model training finished.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Train the Model ---\n",
    "num_epochs = 100 # Max epochs, EarlyStopping will manage it\n",
    "patience = wandb.config[\"epochs\"] # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "print(\"\\nStarting model training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device) # Move data to device\n",
    "        optimizer.zero_grad() # Zero the gradients\n",
    "        outputs = model(inputs) # Forward pass\n",
    "        loss = criterion(outputs, targets) # Calculate loss\n",
    "        loss.backward() # Backward pass (compute gradients)\n",
    "        optimizer.step() # Update weights\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_predictions = []\n",
    "    val_actuals = []\n",
    "    with torch.no_grad(): # Disable gradient calculations during validation\n",
    "        for inputs, targets in test_loader: # Using test_loader for simplicity in this example for validation\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            val_running_loss += val_loss.item() * inputs.size(0)\n",
    "            val_predictions.extend(outputs.cpu().numpy())\n",
    "            val_actuals.extend(targets.cpu().numpy())\n",
    "    \n",
    "    val_epoch_loss = val_running_loss / len(test_dataset) # Use test_dataset for size calculation here\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_epoch_loss:.4f}\")\n",
    "    wandb.log({\n",
    "        \"epoch\" : epoch + 1,\n",
    "        \"train_loss\": epoch_loss,\n",
    "        \"val_loss\": val_epoch_loss\n",
    "    })\n",
    "\n",
    "    # Early Stopping check\n",
    "    if val_epoch_loss < best_val_loss:\n",
    "        best_val_loss = val_epoch_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth') # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} as validation loss did not improve for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "print(\"Model training finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06c5d3ad-46e9-4f0a-a6e9-0121ea8653f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation on Test Set:\n",
      "Mean Squared Error (MSE): 1614.00\n",
      "Mean Absolute Error (MAE): 13.53\n",
      "R-squared (R²): 0.01\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Evaluate the Model on the Test Set (or load best model and evaluate) ---\n",
    "model.load_state_dict(torch.load('best_model.pth'))  # Load the best model weights\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_actuals.append(targets)\n",
    "\n",
    "# Concatenate tensors\n",
    "all_predictions = torch.cat(all_predictions).flatten()\n",
    "all_actuals = torch.cat(all_actuals).flatten()\n",
    "\n",
    "# Compute metrics using PyTorch only\n",
    "mse = torch.mean((all_predictions - all_actuals) ** 2).item()\n",
    "mae = torch.mean(torch.abs(all_predictions - all_actuals)).item()\n",
    "\n",
    "# R² calculation: 1 - SSR/SST\n",
    "ss_res = torch.sum((all_predictions - all_actuals) ** 2)\n",
    "ss_tot = torch.sum((all_actuals - torch.mean(all_actuals)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot).item()\n",
    "\n",
    "print(f\"\\nModel Evaluation on Test Set:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n",
    "\n",
    "wandb.log({\n",
    "    \"test_r2\": r2,\n",
    "    \"MSE\": mse,\n",
    "    \"MAE\": mae,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "24183412-ccbc-4f5c-a566-3ff3721c5124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for sample 0:\n",
      "Actual Upvotes: 2.0\n",
      "Predicted Upvotes: 5.39\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Make Predictions (Optional) ---\n",
    "# Prepare a sample for prediction (e.g., the first sample from the test set)\n",
    "sample_index = 0\n",
    "sample_input_scaled = X_test_tensor[sample_index:sample_index+1]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_upvotes_tensor = model(sample_input_scaled)\n",
    "    predicted_upvotes = predicted_upvotes_tensor.item()  # Convert to scalar\n",
    "\n",
    "# Actual value (assuming y_test is a PyTorch tensor or converted here)\n",
    "actual_upvotes = y_test_tensor[sample_index].item()\n",
    "\n",
    "print(f\"\\nPrediction for sample {sample_index}:\")\n",
    "print(f\"Actual Upvotes: {actual_upvotes}\")\n",
    "print(f\"Predicted Upvotes: {predicted_upvotes:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd51c3f-530b-4a65-8e2a-616ea607500c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
