{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1210f4e0-f0b4-4ebd-9135-c9827ff4476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e873b02b-2799-46dc-8f4e-6fbefceddf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 17005207\n"
     ]
    }
   ],
   "source": [
    "def load_tokens(path):\n",
    "    path = Path(path).expanduser()  # Expand ~\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text.strip().split()\n",
    "\n",
    "words = load_tokens(\"w2v/text8\")\n",
    "print(f\"Total tokens: {len(words)}\")\n",
    "\n",
    "vocab = Counter(words)\n",
    "word_to_idx = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(word_to_idx)\n",
    "\n",
    "# Convert corpus to list of indices\n",
    "corpus = [word_to_idx[w] for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891272e9-dda9-4868-8f38-a465a2b6a14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skipgram_pairs(corpus, window_size=5):\n",
    "    pairs = []\n",
    "    for center_pos in range(len(corpus)):\n",
    "        window = random.randint(1, window_size)\n",
    "        for w in range(-window, window + 1):\n",
    "            context_pos = center_pos + w\n",
    "            if context_pos < 0 or context_pos >= len(corpus) or center_pos == context_pos:\n",
    "                continue\n",
    "            pairs.append((corpus[center_pos], corpus[context_pos]))\n",
    "    return pairs\n",
    "\n",
    "word_freq = np.array([vocab[idx_to_word[i]] for i in range(vocab_size)])\n",
    "unigram_dist = word_freq ** 0.75\n",
    "unigram_dist /= unigram_dist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4004c8b8-1f83-43ed-97ea-88c5d8fb40e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "    def forward(self, center, context, negatives):\n",
    "        center_embed = self.in_embed(center)         # [B, D]\n",
    "        context_embed = self.out_embed(context)      # [B, D]\n",
    "        neg_embed = self.out_embed(negatives)        # [B, K, D]\n",
    "\n",
    "        pos_score = torch.sum(center_embed * context_embed, dim=1)\n",
    "        pos_loss = torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "        neg_score = torch.bmm(neg_embed, center_embed.unsqueeze(2)).squeeze()\n",
    "        neg_loss = torch.sum(torch.log(torch.sigmoid(-neg_score)), dim=1)\n",
    "\n",
    "        return -torch.mean(pos_loss + neg_loss)\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs, unigram_dist, neg_sample_count):\n",
    "        self.pairs = pairs\n",
    "        self.unigram_dist = unigram_dist\n",
    "        self.neg_sample_count = neg_sample_count\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        negatives = np.random.choice(\n",
    "            len(self.unigram_dist), size=self.neg_sample_count, p=self.unigram_dist\n",
    "        )\n",
    "        return (\n",
    "            torch.tensor(center, dtype=torch.long),\n",
    "            torch.tensor(context, dtype=torch.long),\n",
    "            torch.tensor(negatives, dtype=torch.long)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d78909a-2084-4dde-9dc4-08b4a047e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 300           # Higher dimensionality for richer semantic space\n",
    "window_size = 5           # Standard context size; 5 is a good default\n",
    "neg_sample_count = 10     # 5–20 is typical; 10 is a balanced choice\n",
    "batch_size = 1024         # Larger batch size for GPU acceleration (adjust if on CPU)\n",
    "epochs = 10               # For meaningful learning without overfitting\n",
    "\n",
    "pairs = generate_skipgram_pairs(corpus, window_size)\n",
    "dataset = Word2VecDataset(pairs, unigram_dist, neg_sample_count)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = Word2Vec(vocab_size, embed_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "test_words = ['king', 'money', 'computer', '.', '$', 'fitness']  # choose words relevant to your domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800931f-1810-4d03-8b7b-0daff84ab42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_words(target_word, model, word_to_idx, idx_to_word, top_n=5):\n",
    "    if target_word not in word_to_idx:\n",
    "        return f\"'{target_word}' not in vocabulary.\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embed_weights = model.in_embed.weight\n",
    "        norm_embed = F.normalize(embed_weights, dim=1)  # Normalize for cosine similarity\n",
    "\n",
    "        target_idx = word_to_idx[target_word]\n",
    "        target_vec = norm_embed[target_idx]  # [embed_dim]\n",
    "\n",
    "        similarity = torch.matmul(norm_embed, target_vec)\n",
    "        topk = torch.topk(similarity, top_n + 1)\n",
    "        similar_idxs = topk.indices.tolist()\n",
    "\n",
    "        similar_words = [idx_to_word[i] for i in similar_idxs if i != target_idx][:top_n]\n",
    "        return similar_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caff2a6-483b-4e9e-bb55-868eb3630886",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(f\"Batch: {batch}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a96ec6f-1e6c-4165-a5ef-d3c5e572eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, context, negatives in dataloader:\n",
    "        loss = model(center, context, negatives)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # NEW: Validation by finding similar words\n",
    "    print(\"Sample similar words:\")\n",
    "    for word in test_words:\n",
    "        similar = get_similar_words(word, model, word_to_idx, idx_to_word, top_n=5)\n",
    "        print(f\"  {word} → {similar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9eadf-8eae-492d-9598-922e2696dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numpy array of embeddings\n",
    "embeddings = model.in_embed.weight.data.cpu().numpy()\n",
    "\n",
    "# Save to disk if needed\n",
    "np.save(\"word2vec_embeddings.npy\", embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
