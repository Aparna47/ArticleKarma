{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6cd12fc-baa1-424e-9b31-d4f0127ee278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gensim.downloader as api\n",
    "import re\n",
    "import os\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "# For Hacker News dataset\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "torch.set_num_threads(8)  # or up to 10 for M4\n",
    "torch.set_num_interop_threads(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1e65f13-e159-4408-8940-698958eb0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"Inputs1stGen.csv\", low_memory = False, nrows=100000)\n",
    "# Scale the score to 0 to 1\n",
    "df = df.dropna()\n",
    "scaler = MinMaxScaler()\n",
    "df['score_scaled'] = scaler.fit_transform(df[['score']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392656e7-19f7-4eed-91b5-242bc18885f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved GloVe model from disk...\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load GloVe 100 model (download once, save for reuse)\n",
    "model_path = \"glove-wiki-gigaword-100.kv\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading saved GloVe model from disk...\")\n",
    "    glove = KeyedVectors.load(model_path, mmap='r')\n",
    "else:\n",
    "    print(\"Downloading GloVe model...\")\n",
    "    glove = api.load(\"glove-wiki-gigaword-100\")\n",
    "    glove.save(model_path)\n",
    "\n",
    "embedding_dim = glove.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce050b2-29ac-4c90-9916-544ba9774af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Sentence preprocessing + vector averaging\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "def sentence_to_vec_and_len(sentence, model, dim):\n",
    "    tokens = preprocess(sentence)\n",
    "    vectors = [model[word] for word in tokens if word in model.key_to_index]\n",
    "    if not vectors:\n",
    "        return np.zeros(dim), 0\n",
    "    return np.mean(vectors, axis=0), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d294b75-11c8-4e20-ae6e-f332002e1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Vectorise sentences and get lengths\n",
    "vectors = []\n",
    "sentence_lengths = []\n",
    "#for sent in data['sentence']:\n",
    "for sent in df['title']:\n",
    "    vec, length = sentence_to_vec_and_len(sent, glove, embedding_dim)\n",
    "    vectors.append(vec)\n",
    "    sentence_lengths.append(length)\n",
    "\n",
    "df['vector'] = vectors\n",
    "df['length'] = sentence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74076ee-8051-42c1-bcd4-48951440d27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.052608445, 0.26803637, 0.22208327, -0.11418...\n",
       "1        [0.02493834, 0.16105333, -0.080145, -0.1797746...\n",
       "2        [-0.13874899, 0.06257682, 0.44626427, -0.38524...\n",
       "3        [-0.00486115, 0.14162472, 0.46907002, -0.51242...\n",
       "4        [-0.062664725, 0.31050274, 0.20800501, -0.2477...\n",
       "                               ...                        \n",
       "99995    [-0.022318006, -0.17316714, 0.06726713, -0.202...\n",
       "99996    [-0.14299674, 0.13791862, 0.35638934, -0.22488...\n",
       "99997    [-0.058093797, 0.4078598, 0.2369581, -0.535846...\n",
       "99998    [-0.019415336, 0.073686995, 0.12282366, -0.021...\n",
       "99999    [0.13331571, -0.30977002, 0.21067713, -0.40813...\n",
       "Name: vector, Length: 93708, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47b9cf3e-dcb3-479d-9373-807e2eb29f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        11\n",
       "1         6\n",
       "2        12\n",
       "3         7\n",
       "4        11\n",
       "         ..\n",
       "99995     8\n",
       "99996     8\n",
       "99997    10\n",
       "99998     9\n",
       "99999     7\n",
       "Name: length, Length: 93708, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d379098-c023-417c-91f6-550f53604a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Custom Dataset with sentence length\n",
    "class SentenceScoreDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.X_embed = torch.tensor(np.stack(df['vector'].values), dtype=torch.float32)\n",
    "        self.X_len = torch.tensor(df['length'].values, dtype=torch.float32).view(-1, 1)\n",
    "        self.X = torch.cat([self.X_embed, self.X_len], dim=1)\n",
    "        self.y = torch.tensor(df['score_scaled'].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d694c9cc-ee61-450e-a57f-5c1f6c751d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Split and prepare DataLoader\n",
    "#train_df, temp_df = train_test_split(data, test_size=0.4, random_state=42)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = SentenceScoreDataset(train_df)\n",
    "val_ds = SentenceScoreDataset(val_df)\n",
    "test_ds = SentenceScoreDataset(test_df)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=2)\n",
    "test_loader = DataLoader(test_ds, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d1d425-83d9-4031-b605-f75c918d29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: MLP Regression Model\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLPRegressor(embedding_dim + 1)  # +1 for sentence length\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d29244-7f9b-4697-9c43-737469810bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 18.2232, Val Loss: 0.0006, R2: -0.0335, MAE: 0.0072\n",
      "Epoch 2/20, Train Loss: 14.7430, Val Loss: 0.0006, R2: -0.0001, MAE: 0.0076\n",
      "Epoch 3/20, Train Loss: 14.5869, Val Loss: 0.0006, R2: -0.0072, MAE: 0.0070\n",
      "Epoch 4/20, Train Loss: 14.5201, Val Loss: 0.0006, R2: -0.0160, MAE: 0.0112\n",
      "Epoch 5/20, Train Loss: 14.4803, Val Loss: 0.0006, R2: 0.0026, MAE: 0.0079\n",
      "Epoch 6/20, Train Loss: 14.3714, Val Loss: 0.0006, R2: 0.0068, MAE: 0.0088\n",
      "Epoch 7/20, Train Loss: 14.3034, Val Loss: 0.0006, R2: -0.0390, MAE: 0.0124\n",
      "Epoch 8/20, Train Loss: 14.1955, Val Loss: 0.0006, R2: 0.0027, MAE: 0.0093\n",
      "Epoch 9/20, Train Loss: 14.0431, Val Loss: 0.0006, R2: -0.0053, MAE: 0.0076\n",
      "Epoch 10/20, Train Loss: 13.9157, Val Loss: 0.0006, R2: -0.0033, MAE: 0.0084\n",
      "Epoch 11/20, Train Loss: 13.7717, Val Loss: 0.0006, R2: -0.0042, MAE: 0.0082\n",
      "Epoch 12/20, Train Loss: 13.6211, Val Loss: 0.0006, R2: -0.0208, MAE: 0.0104\n",
      "Epoch 13/20, Train Loss: 13.3993, Val Loss: 0.0006, R2: -0.0044, MAE: 0.0093\n",
      "Epoch 14/20, Train Loss: 13.2552, Val Loss: 0.0006, R2: -0.0215, MAE: 0.0108\n",
      "Epoch 15/20, Train Loss: 13.1180, Val Loss: 0.0006, R2: -0.0033, MAE: 0.0089\n",
      "Epoch 16/20, Train Loss: 13.0353, Val Loss: 0.0006, R2: -0.0087, MAE: 0.0092\n",
      "Epoch 17/20, Train Loss: 12.8441, Val Loss: 0.0006, R2: -0.0085, MAE: 0.0086\n",
      "Epoch 18/20, Train Loss: 12.8645, Val Loss: 0.0006, R2: -0.0017, MAE: 0.0083\n",
      "Epoch 19/20, Train Loss: 12.6747, Val Loss: 0.0006, R2: -0.0057, MAE: 0.0086\n",
      "Epoch 20/20, Train Loss: 12.6291, Val Loss: 0.0006, R2: -0.0125, MAE: 0.0077\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Training Loop with Validation\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_val, y_val in val_loader:\n",
    "            preds = model(X_val)\n",
    "            val_loss += criterion(preds, y_val).item()\n",
    "            y_true.extend(y_val.numpy())\n",
    "            y_pred.extend(preds.numpy())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {total_loss:.4f}, Val Loss: {avg_val_loss:.4f}, R2: {r2:.4f}, MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26afa36-c0fb-4fd3-9098-1dcbc0ee4148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results: R2: -0.0187, MAE: 0.0074, MSE: 0.0005\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Final Test Evaluation\n",
    "model.eval()\n",
    "y_true_test, y_pred_test = [], []\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        preds = model(X_test)\n",
    "        y_true_test.extend(y_test.numpy())\n",
    "        y_pred_test.extend(preds.numpy())\n",
    "        \n",
    "r2_test = r2_score(y_true_test, y_pred_test)\n",
    "mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "mse_test = np.mean((np.array(y_true_test) - np.array(y_pred_test)) ** 2)\n",
    "\n",
    "print(f\"\\nTest Results: R2: {r2_test:.4f}, MAE: {mae_test:.4f}, MSE: {mse_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63599c92-d18d-45f3-9102-e680a1e30392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Show sample predictions\n",
    "# print(\"\\nSample Predictions:\")\n",
    "# for true, pred in zip(y_true_test, y_pred_test):\n",
    "#     print(f\"True: {true[0]:.2f}, Predicted: {pred[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4c20ab4-c00d-4277-a49c-b824f2726ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score Prediction from New Sentence:\n",
      "Sentence: Elon Musk just died.\n",
      "Predicted Score: 4.7644\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Generate a score from custom sentence\n",
    "print(\"\\nScore Prediction from New Sentence:\")\n",
    "custom_sentence = \"Elon Musk just died.\"\n",
    "vec, length = sentence_to_vec_and_len(custom_sentence, glove, embedding_dim)\n",
    "input_tensor = torch.tensor(np.append(vec, length), dtype=torch.float32).unsqueeze(0)  # shape (1, dim+1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    scaled_pred = model(input_tensor).item()\n",
    "    predicted_score = scaler.inverse_transform([[scaled_pred]])[0][0]\n",
    "    print(f\"Sentence: {custom_sentence}\\nPredicted Score: {predicted_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9425cc-b564-41e9-9682-3ea7f8a791fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
